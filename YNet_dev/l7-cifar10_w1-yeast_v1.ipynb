{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the data via:\n",
    "\n",
    "    wget http://pjreddie.com/media/files/cifar.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.conv_learner import *\n",
    "PATH = \"datasets/yeast_v2/\"\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('WT', 'mfb1KO')\n",
    "\n",
    "stats = {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# stats = (np.array([0.02171638, 0.00451]), np.array([0.0016155,  0.00146062])) #calculated for yeast_v2\n",
    "\n",
    "# stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "#     tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomFlip()], pad=sz//8)\n",
    "    tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomDihedral()], pad=sz//8) #even without transformations and padding -> failure\n",
    "#     tfms = tfms_from_stats(stats, sz)\n",
    "    return ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)\n",
    "\n",
    "### the eventual sub-function of ImageClassifierData (read_dirs) expects subdirectories for each class: \n",
    "### e.g. all \"test/cat.png\" images should be in a \"cat\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its a DICT!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "please provide transformations for your train and validation sets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-1b1df0a303fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-dbd322c599b2>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(sz, bs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtfms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfms_from_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maug_tfms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRandomDihedral\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#even without transformations and padding -> failure\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     tfms = tfms_from_stats(stats, sz)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImageClassifierData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m### the eventual sub-function of ImageClassifierData (read_dirs) expects subdirectories for each class:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataset.py\u001b[0m in \u001b[0;36mfrom_paths\u001b[1;34m(cls, path, bs, tfms, trn_name, val_name, test_name, test_with_labels, num_workers)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[0mImageClassifierData\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtfms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"please provide transformations for your train and validation sets\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m         \u001b[0mtrn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfolder_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtest_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: please provide transformations for your train and validation sets"
     ]
    }
   ],
   "source": [
    "data = get_data(200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL: no dict\n",
      "normalized with dictnormalized with dicty is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}\n",
      "normalized with dictCALL: no dictCALL: no dictnormalized with dictnormalized with dict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "y is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}y is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}NOT Y\n",
      "\n",
      "\n",
      "NOT YNOT Ynormalized with dictnormalized with dictCALL: no dictCALL: no dict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CALL: no dict\n",
      "normalized with dicty is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}\n",
      "y is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}\n",
      "\n",
      "y is 0; d is {1: ([0.02111, 0.00453], [0.00149, 0.00161]), 0: ([0.0211, 0.0045], [0.00149, 0.00129])}\n",
      "NOT YNOT Y\n",
      "\n",
      "\n",
      "NOT Y\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Normalize' object has no attribute 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-fffeb6d1c3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m# avoid py3.6 issue where queue is infinite and can result in memory exhaustion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[1;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\thread.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataloader.py\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget1item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget1item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataset.py\u001b[0m in \u001b[0;36mget1item\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget1item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataset.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, tfm, x, y)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtfm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtfm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, im, y)\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChannelOrder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfm_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mcompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\transforms.py\u001b[0m in \u001b[0;36mcompose\u001b[1;34m(im, y, fns)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m#pdb.set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'y is {y}; d is {self.d}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NOT Y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Normalize' object has no attribute 'm'"
     ]
    }
   ],
   "source": [
    "x,y=next(iter(data.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data.trn_ds.denorm(x)[3,:,:,1]); #denorm function called has a rollaxis() hence indexing changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(200,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [this notebook](https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb) by our student Kerem Turgutlu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return F.log_softmax(l_x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(SimpleNet([200*200*2, 40,2]), data) #(!) change channel-number & classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleNet(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=80000, out_features=40, bias=True)\n",
       "     (1): Linear(in_features=40, out_features=2, bias=True)\n",
       "   )\n",
       " ), [3200000, 40, 80, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn, [o.numel() for o in learn.model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('Linear-1',\n",
       "               OrderedDict([('input_shape', [-1, 80000]),\n",
       "                            ('output_shape', [-1, 40]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 3200040)])),\n",
       "              ('Linear-2',\n",
       "               OrderedDict([('input_shape', [-1, 40]),\n",
       "                            ('output_shape', [-1, 2]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 82)]))]),\n",
       " Variable containing:\n",
       " -0.6818 -0.7046\n",
       " -0.6935 -0.6928\n",
       " [torch.cuda.FloatTensor of size 2x2 (GPU 0)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(lr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71131d1378e4439b95e068d43991b438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                                                    \n",
      "    0      0.547346   0.668248   0.642857  \n",
      "    1      0.547067   0.668251   0.642857                                                                                                                                                                                                    \n",
      "    2      0.546477   0.66838    0.642857                                                                                                                                                                                                    \n",
      "    3      0.545625   0.668605   0.642857                                                                                                                                                                                                    \n",
      "    4      0.544608   0.668728   0.642857                                                                                                                                                                                                    \n",
      "    5      0.543509   0.668823   0.642857                                                                                                                                                                                                    \n",
      "    6      0.542235   0.668894   0.642857                                                                                                                                                                                                    \n",
      "    7      0.540797   0.668984   0.628571                                                                                                                                                                                                    \n",
      "    8      0.539306   0.669067   0.628571                                                                                                                                                                                                    \n",
      "    9      0.537684   0.669261   0.628571                                                                                                                                                                                                    \n",
      "    10     0.536003   0.669408   0.628571                                                                                                                                                                                                    \n",
      "    11     0.53422    0.669607   0.628571                                                                                                                                                                                                    \n",
      "    12     0.532371   0.669842   0.628571                                                                                                                                                                                                    \n",
      "    13     0.530452   0.670071   0.628571                                                                                                                                                                                                    \n",
      "    14     0.528478   0.670284   0.628571                                                                                                                                                                                                    \n",
      "    15     0.526462   0.670451   0.628571                                                                                                                                                                                                    \n",
      "    16     0.524391   0.670692   0.628571                                                                                                                                                                                                    \n",
      "    17     0.522299   0.670953   0.628571                                                                                                                                                                                                    \n",
      "    18     0.52017    0.67117    0.628571                                                                                                                                                                                                    \n",
      "    19     0.517986   0.67153    0.628571                                                                                                                                                                                                    \n",
      "    20     0.515775   0.671787   0.628571                                                                                                                                                                                                    \n",
      "    21     0.513533   0.672149   0.628571                                                                                                                                                                                                    \n",
      "    22     0.511271   0.67241    0.614286                                                                                                                                                                                                    \n",
      "    23     0.509006   0.672692   0.614286                                                                                                                                                                                                    \n",
      "    24     0.5067     0.672971   0.614286                                                                                                                                                                                                    \n",
      "    25     0.504387   0.673268   0.614286                                                                                                                                                                                                    \n",
      "    26     0.502058   0.673492   0.614286                                                                                                                                                                                                    \n",
      "    27     0.499706   0.673785   0.614286                                                                                                                                                                                                    \n",
      "    28     0.497354   0.674085   0.6                                                                                                                                                                                                         \n",
      "    29     0.494984   0.674316   0.585714                                                                                                                                                                                                    \n",
      "    30     0.492611   0.674592   0.585714                                                                                                                                                                                                    \n",
      "    31     0.49023    0.674872   0.585714                                                                                                                                                                                                    \n",
      "    32     0.487838   0.675184   0.585714                                                                                                                                                                                                    \n",
      "    33     0.485447   0.675489   0.585714                                                                                                                                                                                                    \n",
      "    34     0.483062   0.675866   0.585714                                                                                                                                                                                                    \n",
      "    35     0.480675   0.676241   0.585714                                                                                                                                                                                                    \n",
      "    36     0.478288   0.676637   0.585714                                                                                                                                                                                                    \n",
      "    37     0.475895   0.677075   0.585714                                                                                                                                                                                                    \n",
      "    38     0.473505   0.677394   0.585714                                                                                                                                                                                                    \n",
      "    39     0.471122   0.677764   0.585714                                                                                                                                                                                                    \n",
      "    40     0.468738   0.678198   0.585714                                                                                                                                                                                                    \n",
      "    41     0.466369   0.678687   0.6                                                                                                                                                                                                         \n",
      "    42     0.46399    0.679127   0.6                                                                                                                                                                                                         \n",
      "    43     0.46161    0.679535   0.6                                                                                                                                                                                                         \n",
      "    44     0.459248   0.679937   0.6                                                                                                                                                                                                         \n",
      "    45     0.456888   0.68036    0.6                                                                                                                                                                                                         \n",
      "    46     0.454537   0.680732   0.6                                                                                                                                                                                                         \n",
      "    47     0.452186   0.681118   0.6                                                                                                                                                                                                         \n",
      "    48     0.449852   0.681587   0.6                                                                                                                                                                                                         \n",
      "    49     0.447524   0.682082   0.6                                                                                                                                                                                                         \n",
      "    50     0.445197   0.682552   0.6                                                                                                                                                                                                         \n",
      "    51     0.442879   0.682963   0.6                                                                                                                                                                                                         \n",
      "    52     0.44057    0.683434   0.6                                                                                                                                                                                                         \n",
      "    53     0.438281   0.683923   0.6                                                                                                                                                                                                         \n",
      "    54     0.435998   0.684386   0.6                                                                                                                                                                                                         \n",
      "    55     0.433734   0.68495    0.6                                                                                                                                                                                                         \n",
      "    56     0.431476   0.685413   0.6                                                                                                                                                                                                         \n",
      "    57     0.429224   0.685906   0.614286                                                                                                                                                                                                    \n",
      "    58     0.426994   0.686387   0.614286                                                                                                                                                                                                    \n",
      "    59     0.424768   0.686917   0.628571                                                                                                                                                                                                    \n",
      "    60     0.422554   0.687418   0.628571                                                                                                                                                                                                    \n",
      "    61     0.420356   0.687908   0.628571                                                                                                                                                                                                    \n",
      "    62     0.418167   0.688359   0.628571                                                                                                                                                                                                    \n",
      "    63     0.415987   0.688883   0.628571                                                                                                                                                                                                    \n",
      "    64     0.413817   0.689388   0.628571                                                                                                                                                                                                    \n",
      "    65     0.411651   0.689886   0.628571                                                                                                                                                                                                    \n",
      "    66     0.409504   0.690373   0.628571                                                                                                                                                                                                    \n",
      "    67     0.407367   0.690879   0.628571                                                                                                                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    68     0.405234   0.691344   0.628571                                                                                                                                                                                                    \n",
      "    69     0.403107   0.6918     0.628571                                                                                                                                                                                                    \n",
      "    70     0.400999   0.69233    0.628571                                                                                                                                                                                                    \n",
      "    71     0.398913   0.692815   0.628571                                                                                                                                                                                                    \n",
      "    72     0.396834   0.693281   0.628571                                                                                                                                                                                                    \n",
      "    73     0.394769   0.693722   0.628571                                                                                                                                                                                                    \n",
      "    74     0.392706   0.69413    0.628571                                                                                                                                                                                                    \n",
      "    75     0.39065    0.694595   0.628571                                                                                                                                                                                                    \n",
      "    76     0.388614   0.695152   0.628571                                                                                                                                                                                                    \n",
      "    77     0.386588   0.695653   0.628571                                                                                                                                                                                                    \n",
      "    78     0.384577   0.696146   0.628571                                                                                                                                                                                                    \n",
      "    79     0.382575   0.696607   0.628571                                                                                                                                                                                                    \n",
      "    80     0.380587   0.697117   0.628571                                                                                                                                                                                                    \n",
      "    81     0.378609   0.697593   0.628571                                                                                                                                                                                                    \n",
      "    82     0.376654   0.698041   0.642857                                                                                                                                                                                                    \n",
      "    83     0.374702   0.698514   0.642857                                                                                                                                                                                                    \n",
      "    84     0.372761   0.698943   0.642857                                                                                                                                                                                                    \n",
      "    85     0.370838   0.699379   0.642857                                                                                                                                                                                                    \n",
      "    86     0.368925   0.699921   0.642857                                                                                                                                                                                                    \n",
      "    87     0.367021   0.700374   0.657143                                                                                                                                                                                                    \n",
      "    88     0.365145   0.700854   0.657143                                                                                                                                                                                                    \n",
      "    89     0.363269   0.701306   0.657143                                                                                                                                                                                                    \n",
      "    90     0.361414   0.701707   0.657143                                                                                                                                                                                                    \n",
      "    91     0.35957    0.702152   0.657143                                                                                                                                                                                                    \n",
      "    92     0.357742   0.702604   0.657143                                                                                                                                                                                                    \n",
      "    93     0.355928   0.703103   0.657143                                                                                                                                                                                                    \n",
      "    94     0.354124   0.703581   0.657143                                                                                                                                                                                                    \n",
      "    95     0.352328   0.704022   0.657143                                                                                                                                                                                                    \n",
      "    96     0.35054    0.704462   0.657143                                                                                                                                                                                                    \n",
      "    97     0.348769   0.704907   0.657143                                                                                                                                                                                                    \n",
      "    98     0.347014   0.705428   0.657143                                                                                                                                                                                                    \n",
      "    99     0.345271   0.705864   0.657143                                                                                                                                                                                                    \n",
      "   100     0.343535   0.706358   0.657143                                                                                                                                                                                                    \n",
      "   101     0.341814   0.706824   0.657143                                                                                                                                                                                                    \n",
      "   102     0.340103   0.707304   0.657143                                                                                                                                                                                                    \n",
      "   103     0.338412   0.707721   0.657143                                                                                                                                                                                                    \n",
      "   104     0.33673    0.708203   0.657143                                                                                                                                                                                                    \n",
      "   105     0.335063   0.708698   0.657143                                                                                                                                                                                                    \n",
      "   106     0.333405   0.709196   0.657143                                                                                                                                                                                                    \n",
      "   107     0.331756   0.709614   0.657143                                                                                                                                                                                                    \n",
      "   108     0.330115   0.710066   0.657143                                                                                                                                                                                                    \n",
      "   109     0.32849    0.710568   0.657143                                                                                                                                                                                                    \n",
      "   110     0.326878   0.711038   0.642857                                                                                                                                                                                                    \n",
      "   111     0.325277   0.711466   0.642857                                                                                                                                                                                                    \n",
      "   112     0.323689   0.711811   0.642857                                                                                                                                                                                                    \n",
      "   113     0.322114   0.712269   0.642857                                                                                                                                                                                                    \n",
      "   114     0.320549   0.712722   0.642857                                                                                                                                                                                                    \n",
      "   115     0.318984   0.713177   0.642857                                                                                                                                                                                                    \n",
      "   116     0.317441   0.713625   0.642857                                                                                                                                                                                                    \n",
      "   117     0.315912   0.714113   0.642857                                                                                                                                                                                                    \n",
      "   118     0.314389   0.714559   0.628571                                                                                                                                                                                                    \n",
      "   119     0.312882   0.71503    0.628571                                                                                                                                                                                                    \n",
      "   120     0.311378   0.715503   0.628571                                                                                                                                                                                                    \n",
      "   121     0.309892   0.715997   0.628571                                                                                                                                                                                                    \n",
      "   122     0.308415   0.716424   0.614286                                                                                                                                                                                                    \n",
      "   123     0.306947   0.716888   0.614286                                                                                                                                                                                                    \n",
      "   124     0.305486   0.717376   0.614286                                                                                                                                                                                                    \n",
      "   125     0.304038   0.717807   0.614286                                                                                                                                                                                                    \n",
      "   126     0.302603   0.71832    0.614286                                                                                                                                                                                                    \n",
      "   127     0.301175   0.718748   0.614286                                                                                                                                                                                                    \n",
      "   128     0.299758   0.719193   0.614286                                                                                                                                                                                                    \n",
      "   129     0.298356   0.719624   0.614286                                                                                                                                                                                                    \n",
      "   130     0.296961   0.720114   0.614286                                                                                                                                                                                                    \n",
      "   131     0.295579   0.720592   0.614286                                                                                                                                                                                                    \n",
      "   132     0.294204   0.721062   0.614286                                                                                                                                                                                                    \n",
      "   133     0.292844   0.721543   0.614286                                                                                                                                                                                                    \n",
      "   134     0.29148    0.721961   0.614286                                                                                                                                                                                                    \n",
      "   135     0.290136   0.722491   0.614286                                                                                                                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   136     0.288799   0.722994   0.614286                                                                                                                                                                                                    \n",
      "   137     0.287475   0.723468   0.614286                                                                                                                                                                                                    \n",
      "   138     0.28616    0.723973   0.614286                                                                                                                                                                                                    \n",
      "   139     0.284857   0.724421   0.614286                                                                                                                                                                                                    \n",
      "   140     0.283558   0.724879   0.614286                                                                                                                                                                                                    \n",
      "   141     0.282276   0.725332   0.614286                                                                                                                                                                                                    \n",
      "   142     0.281002   0.725851   0.614286                                                                                                                                                                                                    \n",
      "   143     0.279738   0.726299   0.614286                                                                                                                                                                                                    \n",
      "   144     0.278482   0.72674    0.6                                                                                                                                                                                                         \n",
      "   145     0.277231   0.727154   0.6                                                                                                                                                                                                         \n",
      "   146     0.275994   0.727595   0.6                                                                                                                                                                                                         \n",
      "   147     0.274765   0.728051   0.6                                                                                                                                                                                                         \n",
      "   148     0.273545   0.7285     0.6                                                                                                                                                                                                         \n",
      "   149     0.272331   0.728964   0.6                                                                                                                                                                                                         \n",
      "   150     0.271133   0.729416   0.6                                                                                                                                                                                                         \n",
      "   151     0.269939   0.729889   0.6                                                                                                                                                                                                         \n",
      "   152     0.268762   0.730341   0.6                                                                                                                                                                                                         \n",
      "   153     0.267585   0.730765   0.6                                                                                                                                                                                                         \n",
      "   154     0.26642    0.731225   0.6                                                                                                                                                                                                         \n",
      "   155     0.265263   0.73162    0.6                                                                                                                                                                                                         \n",
      "   156     0.26411    0.732059   0.6                                                                                                                                                                                                         \n",
      "   157     0.26297    0.73252    0.6                                                                                                                                                                                                         \n",
      "   158     0.261833   0.732944   0.6                                                                                                                                                                                                         \n",
      "   159     0.260709   0.733372   0.6                                                                                                                                                                                                         \n",
      "   160     0.25959    0.733733   0.6                                                                                                                                                                                                         \n",
      "   161     0.258481   0.734158   0.6                                                                                                                                                                                                         \n",
      "   162     0.257382   0.734549   0.6                                                                                                                                                                                                         \n",
      "   163     0.256292   0.734985   0.6                                                                                                                                                                                                         \n",
      "   164     0.25521    0.735448   0.6                                                                                                                                                                                                         \n",
      "   165     0.254141   0.735889   0.6                                                                                                                                                                                                         \n",
      "   166     0.253076   0.736369   0.6                                                                                                                                                                                                         \n",
      "   167     0.252021   0.736776   0.6                                                                                                                                                                                                         \n",
      "   168     0.250972   0.737237   0.6                                                                                                                                                                                                         \n",
      "   169     0.249925   0.737651   0.6                                                                                                                                                                                                         \n",
      "   170     0.24889    0.738046   0.6                                                                                                                                                                                                         \n",
      "   171     0.247862   0.738491   0.6                                                                                                                                                                                                         \n",
      "   172     0.246838   0.73887    0.6                                                                                                                                                                                                         \n",
      "   173     0.245819   0.739253   0.6                                                                                                                                                                                                         \n",
      "   174     0.24481    0.739692   0.6                                                                                                                                                                                                         \n",
      "   175     0.243806   0.740119   0.6                                                                                                                                                                                                         \n",
      "   176     0.242811   0.740516   0.6                                                                                                                                                                                                         \n",
      "   177     0.241825   0.740931   0.6                                                                                                                                                                                                         \n",
      "   178     0.240843   0.74137    0.6                                                                                                                                                                                                         \n",
      "   179     0.239872   0.741739   0.6                                                                                                                                                                                                         \n",
      "   180     0.238912   0.742124   0.6                                                                                                                                                                                                         \n",
      "   181     0.23796    0.742535   0.6                                                                                                                                                                                                         \n",
      "   182     0.237011   0.742953   0.6                                                                                                                                                                                                         \n",
      "   183     0.236072   0.74339    0.6                                                                                                                                                                                                         \n",
      "   184     0.235135   0.743803   0.6                                                                                                                                                                                                         \n",
      "   185     0.234199   0.744185   0.6                                                                                                                                                                                                         \n",
      "   186     0.233275   0.744609   0.6                                                                                                                                                                                                         \n",
      "   187     0.232355   0.745058   0.6                                                                                                                                                                                                         \n",
      "   188     0.231445   0.745508   0.6                                                                                                                                                                                                         \n",
      "   189     0.230538   0.745942   0.6                                                                                                                                                                                                         \n",
      "   190     0.22964    0.746374   0.6                                                                                                                                                                                                         \n",
      "   191     0.228747   0.746826   0.6                                                                                                                                                                                                         \n",
      "   192     0.22786    0.74723    0.6                                                                                                                                                                                                         \n",
      "   193     0.226982   0.747652   0.6                                                                                                                                                                                                         \n",
      "   194     0.226107   0.748126   0.6                                                                                                                                                                                                         \n",
      "   195     0.225238   0.74854    0.6                                                                                                                                                                                                         \n",
      "   196     0.224378   0.748951   0.6                                                                                                                                                                                                         \n",
      "   197     0.223521   0.749389   0.6                                                                                                                                                                                                         \n",
      "   198     0.222661   0.749806   0.6                                                                                                                                                                                                         \n",
      "   199     0.221814   0.750208   0.614286                                                                                                                                                                                                    \n",
      "\n",
      "Wall time: 4min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.75021]), 0.6142857074737549]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(lr, 200, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CosAnneal' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f991d3c4ec26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'CosAnneal' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(layers[i], layers[i + 1], kernel_size=3, stride=2)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = F.relu(l(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(ConvNet([2, 20, 40, 80], 2), data)\n",
    "\n",
    "### .from_model_data triggers basicmodel() which forwards to a pytorch's .cuda function to run model on GPU. Hence \n",
    "### an explicit model.cuda is not necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('Conv2d-1',\n",
       "               OrderedDict([('input_shape', [-1, 2, 200, 200]),\n",
       "                            ('output_shape', [-1, 20, 99, 99]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 380)])),\n",
       "              ('Conv2d-2',\n",
       "               OrderedDict([('input_shape', [-1, 20, 99, 99]),\n",
       "                            ('output_shape', [-1, 40, 49, 49]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 7240)])),\n",
       "              ('Conv2d-3',\n",
       "               OrderedDict([('input_shape', [-1, 40, 49, 49]),\n",
       "                            ('output_shape', [-1, 80, 24, 24]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 28880)])),\n",
       "              ('AdaptiveMaxPool2d-4',\n",
       "               OrderedDict([('input_shape', [-1, 80, 24, 24]),\n",
       "                            ('output_shape', [-1, 80, 1, 1]),\n",
       "                            ('nb_params', 0)])),\n",
       "              ('Linear-5',\n",
       "               OrderedDict([('input_shape', [-1, 80]),\n",
       "                            ('output_shape', [-1, 2]),\n",
       "                            ('trainable', True),\n",
       "                            ('nb_params', 162)]))]),\n",
       " Variable containing:\n",
       " -0.5919 -0.8058\n",
       " -0.5827 -0.8173\n",
       " [torch.cuda.FloatTensor of size 2x2 (GPU 0)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary() ### learner.summary is hardcording the number of channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d424943c931643c8b21eba2490194245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                                                    \n",
      "    0      0.731819   0.718615   0.514286  \n",
      "    1      0.731671   0.718472   0.514286                                                                                                                                                                                                    \n",
      "    2      0.731584   0.71827    0.514286                                                                                                                                                                                                    \n",
      "    3      0.731385   0.718003   0.514286                                                                                                                                                                                                    \n",
      "\n",
      "Wall time: 6.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.718]), 0.5142857432365417]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-5, 4, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, ni, nf):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x): return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([ConvLayer(layers[i], layers[i + 1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(ConvNet2([3, 20, 40, 80], 2), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-1, 2, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False, padding=1)\n",
    "        self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds  = x_chan.std (1)[:,None,None]\n",
    "        return (x-self.means) / self.stds *self.m + self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnNet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5, stride=1, padding=2) #(!) changed 3 to 2\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i + 1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l in self.layers: x = l(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(ConvBnNet([10, 20, 40, 80, 160], 2), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d-1',\n",
       "              OrderedDict([('input_shape', [-1, 2, 200, 200]),\n",
       "                           ('output_shape', [-1, 10, 200, 200]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 510)])),\n",
       "             ('Conv2d-2',\n",
       "              OrderedDict([('input_shape', [-1, 10, 200, 200]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1800)])),\n",
       "             ('BnLayer-3',\n",
       "              OrderedDict([('input_shape', [-1, 10, 200, 200]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-4',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 7200)])),\n",
       "             ('BnLayer-5',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-6',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 28800)])),\n",
       "             ('BnLayer-7',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-8',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 115200)])),\n",
       "             ('BnLayer-9',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-10',\n",
       "              OrderedDict([('input_shape', [-1, 160]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 322)]))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(3e-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5715988207bb48b681df5a5170088587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                                                    \n",
      "    0      0.220459   0.261132   0.871429  \n",
      "    1      0.228237   0.255958   0.857143                                                                                                                                                                                                    \n",
      "    2      0.229293   0.24755    0.885714                                                                                                                                                                                                    \n",
      "    3      0.217468   0.258514   0.857143                                                                                                                                                                                                    \n",
      "    4      0.224789   0.269969   0.871429                                                                                                                                                                                                    \n",
      "    5      0.239295   0.511969   0.828571                                                                                                                                                                                                    \n",
      "    6      0.233475   0.64436    0.742857                                                                                                                                                                                                    \n",
      "    7      0.236367   0.302491   0.842857                                                                                                                                                                                                    \n",
      "    8      0.2402     0.267251   0.885714                                                                                                                                                                                                    \n",
      "    9      0.211098   0.256209   0.842857                                                                                                                                                                                                    \n",
      "    10     0.233939   0.287571   0.842857                                                                                                                                                                                                    \n",
      "    11     0.227595   0.307746   0.871429                                                                                                                                                                                                    \n",
      "    12     0.217221   0.303628   0.857143                                                                                                                                                                                                    \n",
      "    13     0.210745   0.299468   0.842857                                                                                                                                                                                                    \n",
      "    14     0.188741   0.273418   0.871429                                                                                                                                                                                                    \n",
      "    15     0.242717   0.263252   0.871429                                                                                                                                                                                                    \n",
      "    16     0.239164   0.266773   0.842857                                                                                                                                                                                                    \n",
      "    17     0.21735    0.274689   0.871429                                                                                                                                                                                                    \n",
      "    18     0.242914   0.251996   0.885714                                                                                                                                                                                                    \n",
      "    19     0.234491   0.363398   0.842857                                                                                                                                                                                                    \n",
      "\n",
      "Wall time: 1min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.3634]), 0.8428571428571429]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-4, 20, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('tmp_batchNorm_aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnNet2(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([BnLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2 in zip(self.layers, self.layers2):\n",
    "            x = l(x)\n",
    "            x = l2(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(ConvBnNet2([10, 20, 40, 80, 160], 10), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 2, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 2), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-5, 2, wds=wd, cycle_len=1, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-5, 3, cycle_len=1, cycle_mult=2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c9cca9435c4e62a7f17ec95e5c4cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                  | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[10, 3, 5, 5], so expected input[140, 2, 200, 200] to have 3 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\fastai\\WORK\\resources\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\fastai\\WORK\\resources\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\fastai\\WORK\\resources\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\fastai\\WORK\\resources\\model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, xs, y, epoch)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-c71d1de2ae88>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 282\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight[10, 3, 5, 5], so expected input[140, 2, 200, 200] to have 3 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "%time learn.fit(1e-5, 20, cycle_len=4, wds=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Resnet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Resnet2(nn.Module):\n",
    "    def __init__(self, layers, c, p=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = BnLayer(3, 16, stride=1, kernel_size=7)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet2([16, 32, 64, 128, 256], 10, 0.2), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wd=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 8, wds=wd, cycle_len=4, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save('tmp4_clr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "preds = np.mean(np.exp(log_preds),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.log_loss(y,preds), accuracy(preds,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(y,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
