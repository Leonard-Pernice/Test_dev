{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.conv_learner import *\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import basename\n",
    "import skimage.external.tifffile as tiff\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"datasets/yeast_v4/\"\n",
    "data_path = Path(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "test_dirs , train_dirs = [], []\n",
    "\n",
    "for ds_dir in data_path.iterdir():\n",
    "    if 'DS_Store' not in str(ds_dir):\n",
    "        for class_dir in ds_dir.iterdir():\n",
    "            if 'DS_Store' not in str(class_dir):\n",
    "                if 'test' in str(class_dir): test_dirs.append(class_dir)\n",
    "                elif 'train' in str(class_dir): train_dirs.append(class_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfb1KO\n",
      "mfb1KO_mmr1KO\n",
      "mmr1KO\n",
      "WT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stats = {}\n",
    "\n",
    "class_dirs = zip(test_dirs,train_dirs)\n",
    "for test, train in class_dirs:\n",
    "    class_images = []\n",
    "    class_name = basename(str(test).split('/')[-1]) #adding \"basename\" function avoids saving entire path; kinda nicer\n",
    "    class_images = []\n",
    "    for dir_ in [test,train]:\n",
    "        # read from each dir and append to the images\n",
    "        for file in dir_.iterdir():\n",
    "            image = tifffile.imread(str(file))\n",
    "            if image.shape[-1] == 200 and image.shape[-2] ==200:\n",
    "                class_images.append(image)\n",
    "            else:\n",
    "                os.remove(str(file)) # get rid of the non square images\n",
    "                print(f\"removed file: {str(file)}\")\n",
    "    \n",
    "    # calc std\n",
    "    print(class_name)\n",
    "    mean, stdev = np.mean(class_images, axis=(0,2,3))/65535 , np.std(class_images, axis=(0,2,3))/65535\n",
    "    stats[class_name] = (mean, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the stats dict\n",
    "\n",
    "with open('./norm_stats.dict','wb') as file:\n",
    "    pickle.dump(stats,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('WT', 'mfb1KO', 'mfb1KO_mmr1KO', 'mmr1KO')\n",
    "PATH = \"datasets/yeast_v4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    create, lbl2index = ImageClassifierData.from_paths_and_stats(PATH, val_name='test', bs=bs)\n",
    "    stats_dict = {lbl2index[key]: val for key, val in stats.items()}\n",
    "    tfms = tfms_from_stats(stats_dict, sz, aug_tfms=[RandomDihedral()], pad=sz//8) #even without transformations and padding -> failure\n",
    "    print(lbl2index)\n",
    "    return create(tfms)\n",
    "\n",
    "### the eventual sub-function of ImageClassifierData (read_dirs) expects subdirectories for each class: \n",
    "### e.g. all \"test/cat.png\" images should be in a \"cat\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inited with dict: {1: (array([0.00794, 0.00476]), array([0.00073, 0.00156])), 2: (array([0.0079 , 0.00474]), array([0.00073, 0.00153])), 3: (array([0.00797, 0.00482]), array([0.00075, 0.00169])), 0: (array([0.00796, 0.00474]), array([0.00074, 0.00146]))}\n",
      "{'WT': 0, 'mfb1KO': 1, 'mfb1KO_mmr1KO': 2, 'mmr1KO': 3}\n"
     ]
    }
   ],
   "source": [
    "data = get_data(200,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(data.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "tiff.imshow(data.trn_ds.denorm(x[idx], y[idx]).squeeze()[:,:,1]); #denorm function called has a rollaxis() hence indexing changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return F.log_softmax(l_x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(SimpleNet([200*200*2, 40, 2]), data) #(!) change channel-number & classes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, [o.numel() for o in learn.model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(lr, 200, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(layers[i], layers[i + 1], kernel_size=5, stride=1, padding=(2,2))\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = F.relu(l(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(ConvNet([2, 20, 40, 80], 4), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[20, 4, 5, 5], so expected input[2, 2, 200, 200] to have 4 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ea6a3d779bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### learner.summary is hardcording the number of channels = 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\conv_learner.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mprecompute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecompute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecompute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(!) changed input channel number. TODO: automatically read channel number.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mmodel_summary\u001b[1;34m(m, input_size)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#(!) modified first arg 3 -> 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#(!) unsure what this is doing, used to not be stored, just executed in orginal code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-e6fbb9769cfd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 282\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight[20, 4, 5, 5], so expected input[2, 2, 200, 200] to have 4 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "learn.summary() ### learner.summary is hardcording the number of channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04f46b388d244b9ad9a945ee77e7f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                            \n",
      "    0      1.322663   1.423939   0.228571  \n",
      "    1      1.326295   1.423939   0.228571                                                                                                                                                                            \n",
      "    2      1.324928   1.423939   0.228571                                                                                                                                                                            \n",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 6/10 [00:05<00:03,  1.03it/s, loss=1.33]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, xs, y, epoch)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mcopy_fp32_to_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time learn.fit(lr, 10, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet_with_Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False, padding=1)\n",
    "        self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds  = x_chan.std (1)[:,None,None]\n",
    "        return (x-self.means) / self.stds *self.m + self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 4), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d-1',\n",
       "              OrderedDict([('input_shape', [-1, 2, 200, 200]),\n",
       "                           ('output_shape', [-1, 10, 200, 200]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 510)])),\n",
       "             ('Conv2d-2',\n",
       "              OrderedDict([('input_shape', [-1, 10, 200, 200]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1800)])),\n",
       "             ('BnLayer-3',\n",
       "              OrderedDict([('input_shape', [-1, 10, 200, 200]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-4',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 3600)])),\n",
       "             ('ResnetLayer-5',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-6',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 3600)])),\n",
       "             ('ResnetLayer-7',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 20, 100, 100]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-8',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 7200)])),\n",
       "             ('BnLayer-9',\n",
       "              OrderedDict([('input_shape', [-1, 20, 100, 100]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-10',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 14400)])),\n",
       "             ('ResnetLayer-11',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-12',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 14400)])),\n",
       "             ('ResnetLayer-13',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 40, 50, 50]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-14',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 28800)])),\n",
       "             ('BnLayer-15',\n",
       "              OrderedDict([('input_shape', [-1, 40, 50, 50]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-16',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 57600)])),\n",
       "             ('ResnetLayer-17',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-18',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 57600)])),\n",
       "             ('ResnetLayer-19',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 80, 25, 25]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-20',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 115200)])),\n",
       "             ('BnLayer-21',\n",
       "              OrderedDict([('input_shape', [-1, 80, 25, 25]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-22',\n",
       "              OrderedDict([('input_shape', [-1, 160, 13, 13]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 230400)])),\n",
       "             ('ResnetLayer-23',\n",
       "              OrderedDict([('input_shape', [-1, 160, 13, 13]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-24',\n",
       "              OrderedDict([('input_shape', [-1, 160, 13, 13]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 230400)])),\n",
       "             ('ResnetLayer-25',\n",
       "              OrderedDict([('input_shape', [-1, 160, 13, 13]),\n",
       "                           ('output_shape', [-1, 160, 13, 13]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-26',\n",
       "              OrderedDict([('input_shape', [-1, 160]),\n",
       "                           ('output_shape', [-1, 4]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 644)]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4034d4226c411e9166b7acb94718ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                            \n",
      "    0      48.563653  65.042301  0.228571  \n",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 6/10 [00:03<00:02,  1.95it/s, loss=40.7]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, xs, y, epoch)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mcopy_fp32_to_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8087a645cc74d4d94ef959394d85d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                            \n",
      "    0      1.001357   1.401626   0.47619   \n",
      "    1      1.01478    1.333267   0.428571                                                                                                                                                                            \n",
      "    2      1.049442   1.210411   0.447619                                                                                                                                                                            \n",
      "    3      1.037015   1.283607   0.47619                                                                                                                                                                             \n",
      "    4      1.029389   1.289065   0.495238                                                                                                                                                                            \n",
      "    5      1.032554   1.513644   0.371429                                                                                                                                                                            \n",
      "    6      1.036173   1.178574   0.514286                                                                                                                                                                            \n",
      "    7      1.02008    1.20606    0.438095                                                                                                                                                                            \n",
      "    8      1.006848   1.223289   0.514286                                                                                                                                                                            \n",
      "    9      0.998518   1.17282    0.514286                                                                                                                                                                            \n",
      "    10     0.985133   1.150259   0.514286                                                                                                                                                                            \n",
      "    11     0.968344   1.205815   0.495238                                                                                                                                                                            \n",
      "    12     0.955395   1.332061   0.457143                                                                                                                                                                            \n",
      "    13     0.965552   1.210558   0.533333                                                                                                                                                                            \n",
      "    14     0.957732   1.285588   0.466667                                                                                                                                                                            \n",
      "    15     0.945926   1.174511   0.485714                                                                                                                                                                            \n",
      "    16     0.941816   1.39032    0.447619                                                                                                                                                                            \n",
      "    17     0.949732   1.315779   0.495238                                                                                                                                                                            \n",
      "    18     0.951859   1.27544    0.457143                                                                                                                                                                            \n",
      "    19     0.937565   1.185845   0.504762                                                                                                                                                                            \n",
      "    20     0.951233   1.152737   0.514286                                                                                                                                                                            \n",
      "    21     0.946213   1.137064   0.485714                                                                                                                                                                            \n",
      "    22     0.931712   1.181442   0.47619                                                                                                                                                                             \n",
      "    23     0.927862   1.217635   0.504762                                                                                                                                                                            \n",
      "    24     0.919357   1.196004   0.514286                                                                                                                                                                            \n",
      "    25     0.92445    1.131113   0.514286                                                                                                                                                                            \n",
      "    26     0.917511   1.12574    0.485714                                                                                                                                                                            \n",
      "    27     0.899155   1.189465   0.495238                                                                                                                                                                            \n",
      "    28     0.908381   1.324667   0.495238                                                                                                                                                                            \n",
      "    29     0.9154     1.14361    0.561905                                                                                                                                                                            \n",
      "    30     0.893192   1.068848   0.52381                                                                                                                                                                             \n",
      "    31     0.884551   1.086249   0.52381                                                                                                                                                                             \n",
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.08625]), 0.5238095246610187]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-2, 8, wds=wd, cycle_len=4, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda525db681a4614a7fa5036ea7f13cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                            \n",
      "    0      15.853056  12.190327  0.32381   \n",
      "    1      11.521627  3.462061   0.238095                                                                                                                                                                            \n",
      "    2      7.672445   1.542595   0.27619                                                                                                                                                                             \n",
      "    3      5.674883   1.355551   0.4                                                                                                                                                                                 \n",
      "    4      4.44524    1.45432    0.295238                                                                                                                                                                            \n",
      "    5      3.683185   1.591556   0.32381                                                                                                                                                                             \n",
      "    6      3.117972   1.373948   0.333333                                                                                                                                                                            \n",
      "    7      2.702504   1.539505   0.295238                                                                                                                                                                            \n",
      "    8      2.442438   1.432909   0.371429                                                                                                                                                                            \n",
      "    9      2.190076   1.36616    0.380952                                                                                                                                                                            \n",
      "    10     2.011154   1.386147   0.342857                                                                                                                                                                            \n",
      "    11     1.90024    1.345811   0.4                                                                                                                                                                                 \n",
      "    12     1.752274   1.325634   0.380952                                                                                                                                                                            \n",
      "    13     1.636487   1.393261   0.409524                                                                                                                                                                            \n",
      "    14     1.534192   1.253923   0.409524                                                                                                                                                                            \n",
      "    15     1.454962   1.252702   0.371429                                                                                                                                                                            \n",
      "    16     1.378917   1.353574   0.438095                                                                                                                                                                            \n",
      "    17     1.311185   1.304888   0.409524                                                                                                                                                                            \n",
      "    18     1.288983   1.250824   0.390476                                                                                                                                                                            \n",
      "    19     1.253296   1.299131   0.390476                                                                                                                                                                            \n",
      "    20     1.228254   1.378068   0.371429                                                                                                                                                                            \n",
      "    21     1.196275   1.380061   0.380952                                                                                                                                                                            \n",
      "    22     1.157575   1.227397   0.447619                                                                                                                                                                            \n",
      "    23     1.128742   1.294768   0.409524                                                                                                                                                                            \n",
      "    24     1.104562   1.256625   0.390476                                                                                                                                                                            \n",
      "    25     1.081044   1.183902   0.419048                                                                                                                                                                            \n",
      "    26     1.059565   1.262986   0.380952                                                                                                                                                                            \n",
      "    27     1.03821    1.19062    0.457143                                                                                                                                                                            \n",
      "    28     1.019117   1.27593    0.466667                                                                                                                                                                            \n",
      "    29     1.015837   1.218731   0.428571                                                                                                                                                                            \n",
      "    30     1.009736   1.464781   0.380952                                                                                                                                                                            \n",
      "    31     1.002302   1.201715   0.457143                                                                                                                                                                            \n",
      "    32     0.98869    1.147013   0.47619                                                                                                                                                                             \n",
      "    33     0.979413   1.234974   0.4                                                                                                                                                                                 \n",
      "    34     0.966875   1.181228   0.419048                                                                                                                                                                            \n",
      "    35     0.95931    1.224626   0.52381                                                                                                                                                                             \n",
      "    36     0.962096   1.477044   0.371429                                                                                                                                                                            \n",
      "    37     0.970686   1.384385   0.457143                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38     0.965495   1.165474   0.52381                                                                                                                                                                             \n",
      "    39     0.949642   1.241759   0.466667                                                                                                                                                                            \n",
      "    40     0.942071   1.15547    0.447619                                                                                                                                                                            \n",
      "    41     0.934215   1.20334    0.514286                                                                                                                                                                            \n",
      "    42     0.940196   1.146647   0.485714                                                                                                                                                                            \n",
      "    43     0.94432    1.178352   0.495238                                                                                                                                                                            \n",
      "    44     0.943565   1.16972    0.495238                                                                                                                                                                            \n",
      "    45     0.935706   1.243643   0.447619                                                                                                                                                                            \n",
      "    46     0.929269   1.166353   0.485714                                                                                                                                                                            \n",
      "    47     0.934969   1.153195   0.514286                                                                                                                                                                            \n",
      "    48     0.946724   1.235711   0.485714                                                                                                                                                                            \n",
      "    49     0.941034   1.292778   0.485714                                                                                                                                                                            \n",
      "    50     0.959724   1.298482   0.428571                                                                                                                                                                            \n",
      "    51     0.963674   1.188148   0.47619                                                                                                                                                                             \n",
      "    52     0.945636   1.187089   0.504762                                                                                                                                                                            \n",
      "    53     0.928926   1.052063   0.571429                                                                                                                                                                            \n",
      "    54     0.911895   1.102633   0.514286                                                                                                                                                                            \n",
      "    55     0.898441   1.116605   0.542857                                                                                                                                                                            \n",
      "    56     0.890507   1.226285   0.47619                                                                                                                                                                             \n",
      "    57     0.888133   1.221556   0.495238                                                                                                                                                                            \n",
      "    58     0.883578   1.247292   0.533333                                                                                                                                                                            \n",
      "    59     0.896212   1.103271   0.52381                                                                                                                                                                             \n",
      "    60     0.882191   1.061268   0.552381                                                                                                                                                                            \n",
      "    61     0.86291    1.08747    0.514286                                                                                                                                                                            \n",
      "    62     0.846799   1.16273    0.495238                                                                                                                                                                            \n",
      "    63     0.843726   1.173749   0.495238                                                                                                                                                                            \n",
      "    64     0.833445   1.098694   0.514286                                                                                                                                                                            \n",
      "    65     0.826252   1.100995   0.542857                                                                                                                                                                            \n",
      "    66     0.813818   1.026661   0.6                                                                                                                                                                                 \n",
      "    67     0.811504   1.067658   0.542857                                                                                                                                                                            \n",
      "    68     0.804537   1.072203   0.552381                                                                                                                                                                            \n",
      "    69     0.79475    1.121152   0.52381                                                                                                                                                                             \n",
      "    70     0.779535   1.091565   0.52381                                                                                                                                                                             \n",
      "    71     0.777338   1.099534   0.495238                                                                                                                                                                            \n",
      "    72     0.766578   1.104949   0.52381                                                                                                                                                                             \n",
      "    73     0.755481   1.076622   0.571429                                                                                                                                                                            \n",
      "    74     0.745948   1.107958   0.580952                                                                                                                                                                            \n",
      "    75     0.738316   1.173614   0.495238                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    76     0.733638   1.114137   0.561905                                                                                                                                                                            \n",
      "    77     0.728903   1.006554   0.590476                                                                                                                                                                            \n",
      "    78     0.728091   1.009754   0.590476                                                                                                                                                                            \n",
      "    79     0.718118   1.001939   0.561905                                                                                                                                                                            \n",
      "    80     0.710953   1.09576    0.542857                                                                                                                                                                            \n",
      "    81     0.708454   1.029971   0.590476                                                                                                                                                                            \n",
      "    82     0.712354   1.281855   0.514286                                                                                                                                                                            \n",
      "    83     0.724735   1.130404   0.52381                                                                                                                                                                             \n",
      "    84     0.743309   1.361896   0.485714                                                                                                                                                                            \n",
      "    85     0.757809   1.168925   0.485714                                                                                                                                                                            \n",
      "    86     0.746913   1.067509   0.552381                                                                                                                                                                            \n",
      "    87     0.74714    1.093598   0.52381                                                                                                                                                                             \n",
      "    88     0.745037   0.998873   0.552381                                                                                                                                                                            \n",
      "    89     0.759109   1.521188   0.466667                                                                                                                                                                            \n",
      "    90     0.772417   1.302407   0.52381                                                                                                                                                                             \n",
      "    91     0.763879   1.069011   0.571429                                                                                                                                                                            \n",
      "    92     0.756378   1.032298   0.561905                                                                                                                                                                            \n",
      "    93     0.760895   1.054397   0.6                                                                                                                                                                                 \n",
      "    94     0.75138    1.069762   0.571429                                                                                                                                                                            \n",
      "    95     0.740677   0.976258   0.580952                                                                                                                                                                            \n",
      "    96     0.729342   1.035251   0.580952                                                                                                                                                                            \n",
      "    97     0.719761   1.031018   0.542857                                                                                                                                                                            \n",
      "    98     0.733393   0.947063   0.609524                                                                                                                                                                            \n",
      "    99     0.741429   0.927722   0.628571                                                                                                                                                                            \n",
      "   100     0.734487   0.947237   0.561905                                                                                                                                                                            \n",
      "   101     0.731733   1.064433   0.6                                                                                                                                                                                 \n",
      "   102     0.736576   1.054414   0.609524                                                                                                                                                                            \n",
      "   103     0.722104   1.013687   0.571429                                                                                                                                                                            \n",
      "   104     0.70822    0.967543   0.647619                                                                                                                                                                            \n",
      "   105     0.689229   1.0032     0.580952                                                                                                                                                                            \n",
      "   106     0.682116   0.946411   0.6                                                                                                                                                                                 \n",
      "   107     0.670426   0.96886    0.638095                                                                                                                                                                            \n",
      "   108     0.661656   1.003905   0.6                                                                                                                                                                                 \n",
      "   109     0.664067   1.014649   0.609524                                                                                                                                                                            \n",
      "   110     0.662663   1.091944   0.580952                                                                                                                                                                            \n",
      "   111     0.647015   0.995228   0.561905                                                                                                                                                                            \n",
      "   112     0.632042   0.987782   0.619048                                                                                                                                                                            \n",
      "   113     0.629195   1.064396   0.590476                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   114     0.628593   0.948173   0.628571                                                                                                                                                                            \n",
      "   115     0.624901   1.081444   0.580952                                                                                                                                                                            \n",
      "   116     0.621572   1.172631   0.52381                                                                                                                                                                             \n",
      "   117     0.610335   1.001415   0.571429                                                                                                                                                                            \n",
      "   118     0.609613   0.969645   0.6                                                                                                                                                                                 \n",
      "   119     0.595698   0.973763   0.590476                                                                                                                                                                            \n",
      "   120     0.596805   1.313056   0.514286                                                                                                                                                                            \n",
      "   121     0.597945   0.936683   0.628571                                                                                                                                                                            \n",
      "   122     0.586555   0.965838   0.647619                                                                                                                                                                            \n",
      "   123     0.58764    1.02568    0.561905                                                                                                                                                                            \n",
      "   124     0.583726   1.016622   0.609524                                                                                                                                                                            \n",
      "   125     0.598522   1.227308   0.590476                                                                                                                                                                            \n",
      "   126     0.604738   1.06776    0.561905                                                                                                                                                                            \n",
      "   127     0.617697   1.081717   0.590476                                                                                                                                                                            \n",
      "   128     0.610914   1.192641   0.552381                                                                                                                                                                            \n",
      "   129     0.62928    1.226781   0.52381                                                                                                                                                                             \n",
      "   130     0.63477    0.966058   0.619048                                                                                                                                                                            \n",
      "   131     0.638089   1.49277    0.485714                                                                                                                                                                            \n",
      "   132     0.639848   1.035979   0.580952                                                                                                                                                                            \n",
      "   133     0.642275   1.013446   0.571429                                                                                                                                                                            \n",
      "   134     0.636277   1.099059   0.533333                                                                                                                                                                            \n",
      "   135     0.627185   1.002336   0.619048                                                                                                                                                                            \n",
      "   136     0.62207    0.969953   0.6                                                                                                                                                                                 \n",
      "   137     0.620186   1.066787   0.542857                                                                                                                                                                            \n",
      "   138     0.618084   1.013905   0.552381                                                                                                                                                                            \n",
      "   139     0.606004   0.921249   0.619048                                                                                                                                                                            \n",
      "   140     0.59503    0.962039   0.628571                                                                                                                                                                            \n",
      "   141     0.602468   0.980409   0.590476                                                                                                                                                                            \n",
      "   142     0.599695   0.948867   0.657143                                                                                                                                                                            \n",
      "   143     0.599402   1.252971   0.590476                                                                                                                                                                            \n",
      "   144     0.583501   1.021841   0.638095                                                                                                                                                                            \n",
      "   145     0.574456   1.001243   0.638095                                                                                                                                                                            \n",
      "   146     0.5684     1.012741   0.638095                                                                                                                                                                            \n",
      "   147     0.565259   1.122886   0.590476                                                                                                                                                                            \n",
      "   148     0.558904   1.130981   0.580952                                                                                                                                                                            \n",
      "   149     0.550093   0.975552   0.67619                                                                                                                                                                             \n",
      "   150     0.541398   0.95605    0.647619                                                                                                                                                                            \n",
      "   151     0.52965    0.970519   0.647619                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   152     0.524013   0.941924   0.628571                                                                                                                                                                            \n",
      "   153     0.511051   0.944797   0.609524                                                                                                                                                                            \n",
      "   154     0.508315   1.258017   0.609524                                                                                                                                                                            \n",
      "   155     0.500762   0.964263   0.657143                                                                                                                                                                            \n",
      "   156     0.501229   1.103468   0.580952                                                                                                                                                                            \n",
      "   157     0.499746   0.990054   0.67619                                                                                                                                                                             \n",
      "   158     0.491921   1.077913   0.609524                                                                                                                                                                            \n",
      "   159     0.484116   0.950434   0.609524                                                                                                                                                                            \n",
      "   160     0.481785   1.008317   0.628571                                                                                                                                                                            \n",
      "   161     0.487109   0.969477   0.628571                                                                                                                                                                            \n",
      "   162     0.486565   0.934847   0.638095                                                                                                                                                                            \n",
      "   163     0.509055   1.177881   0.561905                                                                                                                                                                            \n",
      "   164     0.531566   1.621966   0.52381                                                                                                                                                                             \n",
      "   165     0.543558   1.023053   0.590476                                                                                                                                                                            \n",
      "   166     0.544689   0.98844    0.638095                                                                                                                                                                            \n",
      "   167     0.546054   1.197166   0.561905                                                                                                                                                                            \n",
      "   168     0.545738   1.005567   0.638095                                                                                                                                                                            \n",
      "   169     0.537739   0.943939   0.657143                                                                                                                                                                            \n",
      "   170     0.535893   1.056255   0.6                                                                                                                                                                                 \n",
      "   171     0.530375   1.032008   0.580952                                                                                                                                                                            \n",
      "   172     0.560263   1.167262   0.580952                                                                                                                                                                            \n",
      "   173     0.558449   1.210784   0.552381                                                                                                                                                                            \n",
      "   174     0.558441   0.964079   0.609524                                                                                                                                                                            \n",
      "   175     0.553255   0.90267    0.619048                                                                                                                                                                            \n",
      "   176     0.539674   0.994606   0.647619                                                                                                                                                                            \n",
      "   177     0.52928    1.04912    0.619048                                                                                                                                                                            \n",
      "   178     0.521815   0.949028   0.638095                                                                                                                                                                            \n",
      "   179     0.521747   1.002177   0.619048                                                                                                                                                                            \n",
      "   180     0.521837   0.947888   0.685714                                                                                                                                                                            \n",
      "   181     0.522861   1.01597    0.6                                                                                                                                                                                 \n",
      "   182     0.516287   1.015528   0.619048                                                                                                                                                                            \n",
      "   183     0.51159    0.928821   0.647619                                                                                                                                                                            \n",
      "   184     0.511044   1.321687   0.495238                                                                                                                                                                            \n",
      "   185     0.500164   1.008932   0.619048                                                                                                                                                                            \n",
      "   186     0.495804   1.051657   0.619048                                                                                                                                                                            \n",
      "   187     0.48685    0.998352   0.628571                                                                                                                                                                            \n",
      "   188     0.478421   1.018731   0.628571                                                                                                                                                                            \n",
      "   189     0.477825   1.052612   0.628571                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   190     0.472134   1.065513   0.609524                                                                                                                                                                            \n",
      "   191     0.458726   0.991573   0.580952                                                                                                                                                                            \n",
      "   192     0.453273   1.06797    0.609524                                                                                                                                                                            \n",
      "   193     0.449901   0.991386   0.628571                                                                                                                                                                            \n",
      "   194     0.452179   0.95801    0.666667                                                                                                                                                                            \n",
      "   195     0.441335   0.986623   0.628571                                                                                                                                                                            \n",
      "   196     0.434871   1.006113   0.628571                                                                                                                                                                            \n",
      "   197     0.435611   1.005516   0.619048                                                                                                                                                                            \n",
      "   198     0.43191    1.07213    0.6                                                                                                                                                                                 \n",
      "   199     0.424144   1.052749   0.609524                                                                                                                                                                            \n",
      "   200     0.415681   1.072348   0.6                                                                                                                                                                                 \n",
      "   201     0.414736   0.933822   0.638095                                                                                                                                                                            \n",
      "   202     0.411075   1.033593   0.590476                                                                                                                                                                            \n",
      "   203     0.421217   1.109547   0.6                                                                                                                                                                                 \n",
      "   204     0.441343   1.088503   0.580952                                                                                                                                                                            \n",
      "   205     0.44866    1.250584   0.571429                                                                                                                                                                            \n",
      "   206     0.456283   1.201599   0.619048                                                                                                                                                                            \n",
      "   207     0.461985   1.033262   0.647619                                                                                                                                                                            \n",
      "   208     0.466202   0.99099    0.666667                                                                                                                                                                            \n",
      "   209     0.472984   0.932342   0.666667                                                                                                                                                                            \n",
      "   210     0.483789   1.185582   0.590476                                                                                                                                                                            \n",
      "   211     0.469472   0.961335   0.657143                                                                                                                                                                            \n",
      "   212     0.463968   0.992238   0.6                                                                                                                                                                                 \n",
      "   213     0.470299   1.021271   0.609524                                                                                                                                                                            \n",
      "   214     0.477038   0.980741   0.590476                                                                                                                                                                            \n",
      "   215     0.483868   1.364937   0.542857                                                                                                                                                                            \n",
      "   216     0.472832   0.987489   0.619048                                                                                                                                                                            \n",
      "   217     0.467041   0.875101   0.695238                                                                                                                                                                            \n",
      "   218     0.46932    1.108445   0.580952                                                                                                                                                                            \n",
      "   219     0.464759   1.214318   0.542857                                                                                                                                                                            \n",
      "   220     0.454076   0.849226   0.695238                                                                                                                                                                            \n",
      "   221     0.441347   1.710666   0.52381                                                                                                                                                                             \n",
      "   222     0.449126   1.069738   0.609524                                                                                                                                                                            \n",
      "   223     0.437509   0.960483   0.666667                                                                                                                                                                            \n",
      "   224     0.432681   1.095024   0.657143                                                                                                                                                                            \n",
      "   225     0.429161   0.979043   0.6                                                                                                                                                                                 \n",
      "   226     0.413591   0.9919     0.628571                                                                                                                                                                            \n",
      "   227     0.406066   0.97613    0.647619                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   228     0.408102   1.06222    0.628571                                                                                                                                                                            \n",
      "   229     0.399172   0.987417   0.609524                                                                                                                                                                            \n",
      "   230     0.391128   0.914872   0.638095                                                                                                                                                                            \n",
      "   231     0.388556   0.975346   0.628571                                                                                                                                                                            \n",
      "   232     0.382658   0.938573   0.666667                                                                                                                                                                            \n",
      "   233     0.385082   1.0404     0.628571                                                                                                                                                                            \n",
      "   234     0.392196   1.232873   0.619048                                                                                                                                                                            \n",
      "   235     0.381395   0.950653   0.647619                                                                                                                                                                            \n",
      "   236     0.380899   0.979417   0.638095                                                                                                                                                                            \n",
      "   237     0.381868   1.137701   0.657143                                                                                                                                                                            \n",
      "   238     0.372488   0.959354   0.647619                                                                                                                                                                            \n",
      "   239     0.361932   0.996209   0.609524                                                                                                                                                                            \n",
      "   240     0.355526   0.915982   0.628571                                                                                                                                                                            \n",
      "   241     0.348536   0.946234   0.657143                                                                                                                                                                            \n",
      "   242     0.353143   0.976251   0.666667                                                                                                                                                                            \n",
      "   243     0.362763   1.012573   0.647619                                                                                                                                                                            \n",
      "   244     0.37709    1.023941   0.6                                                                                                                                                                                 \n",
      "   245     0.386465   1.117692   0.6                                                                                                                                                                                 \n",
      "   246     0.408448   1.258779   0.609524                                                                                                                                                                            \n",
      "   247     0.431746   0.957672   0.638095                                                                                                                                                                            \n",
      "   248     0.437271   1.13368    0.561905                                                                                                                                                                            \n",
      "   249     0.448659   0.946923   0.666667                                                                                                                                                                            \n",
      " 30%|█████████████████████████████████████████████████▌                                                                                                                   | 3/10 [00:03<00:08,  1.28s/it, loss=0.447]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, xs, y, epoch)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mcopy_fp32_to_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time learn.fit(1e-2, 8, wds=wd, cycle_len=40, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0d59ed9d0d418e83e88bf52539958e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=320), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                                                                                                                            \n",
      "    0      0.41657    1.03402    0.619048  \n",
      "    1      0.397302   0.906338   0.647619                                                                                                                                                                            \n",
      "    2      0.381994   0.849025   0.647619                                                                                                                                                                            \n",
      "    3      0.366051   0.887515   0.67619                                                                                                                                                                             \n",
      "    4      0.348402   0.940935   0.657143                                                                                                                                                                            \n",
      "    5      0.340963   0.927903   0.685714                                                                                                                                                                            \n",
      "    6      0.343927   0.89243    0.657143                                                                                                                                                                            \n",
      "    7      0.341238   1.186194   0.638095                                                                                                                                                                            \n",
      "    8      0.340491   1.092923   0.638095                                                                                                                                                                            \n",
      "    9      0.334453   0.927322   0.666667                                                                                                                                                                            \n",
      "    10     0.333208   0.918816   0.666667                                                                                                                                                                            \n",
      "    11     0.321172   0.937443   0.628571                                                                                                                                                                            \n",
      "    12     0.319138   0.989746   0.647619                                                                                                                                                                            \n",
      "    13     0.324547   0.923786   0.666667                                                                                                                                                                            \n",
      "    14     0.321311   0.978654   0.628571                                                                                                                                                                            \n",
      "    15     0.319785   0.95636    0.657143                                                                                                                                                                            \n",
      "    16     0.315716   0.93283    0.67619                                                                                                                                                                             \n",
      "    17     0.315248   1.130379   0.609524                                                                                                                                                                            \n",
      "    18     0.311196   1.016309   0.67619                                                                                                                                                                             \n",
      "    19     0.308724   0.998229   0.647619                                                                                                                                                                            \n",
      "    20     0.30557    0.895317   0.67619                                                                                                                                                                             \n",
      "    21     0.308217   0.920497   0.647619                                                                                                                                                                            \n",
      "    22     0.310258   0.931083   0.657143                                                                                                                                                                            \n",
      "    23     0.302104   1.038101   0.638095                                                                                                                                                                            \n",
      "    24     0.301063   1.005708   0.657143                                                                                                                                                                            \n",
      "    25     0.298744   0.941547   0.657143                                                                                                                                                                            \n",
      "    26     0.289996   0.976539   0.666667                                                                                                                                                                            \n",
      "    27     0.289402   0.900277   0.657143                                                                                                                                                                            \n",
      "    28     0.289898   0.976437   0.647619                                                                                                                                                                            \n",
      "    29     0.297018   1.122277   0.609524                                                                                                                                                                            \n",
      "    30     0.296333   1.021458   0.666667                                                                                                                                                                            \n",
      "  0%|                                                                                                                                                                                         | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall_val\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIterBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    925\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DEV\\GitHub\\Test_dev\\YNet_dev\\resources\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m# avoid py3.6 issue where queue is infinite and can result in memory exhaustion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[1;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\fastai\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time learn.fit(1e-3, 8, wds=wd, cycle_len=40, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('tmp_resnet_clr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
