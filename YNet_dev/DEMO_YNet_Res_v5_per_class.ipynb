{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YNet - Dataset 5:\n",
    "\n",
    "Data from Experiment (2), Mitochondria = Cit1-mCherry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from resources.conv_learner import *\n",
    "from resources.plots import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"datasets/yeast_v5.1/\"\n",
    "data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ('WT', 'mfb1KO', 'mfb1KO_mmr1KO', 'mmr1KO', 'mmm1KO_230', 'num1_110' )\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating normalization statistics\n",
    "\n",
    "Note that we are setting up train & val data, as well as test. Within test, we are here including a mutant cell type that the model never trains on. The idea is to use to the feature space developed during training to evaluate novel cell types by similarity to the landmarks that the model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_name = \"yeast_v5_per_class.dict\"\n",
    "classes = Statistics.source_class(data_path)\n",
    "\n",
    "train_val = zip(classes['train'], classes['val'])\n",
    "Xtest = zip(classes['test'])\n",
    " \n",
    "main_stats = Statistics.per_class(train_val, save_name=stats_name)\n",
    "test_stats = Statistics.per_class(Xtest, save_name=stats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for keys in main_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {main_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for keys in test_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {test_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfms_for_test(stats, sz):\n",
    "    test_norm = Normalize(stats)\n",
    "    test_denorm = Denormalize(stats)\n",
    "    val_crop = CropType.NO\n",
    "    test_tfms = image_gen(test_norm, test_denorm,sz, crop_type=val_crop)\n",
    "    return test_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: str, sz, bs):\n",
    "    create, lbl2index, lbl2index_test  = ImageClassifierData.prepare_from_path(path, val_name='val', bs=bs)\n",
    "    main_stats_X = {lbl2index[key]: val for key, val in main_stats.items()}\n",
    "    test_stats_X= {lbl2index_test[key]: val for key, val in lbl2index_test.items()}\n",
    "    tfms = tfms_from_stats(main_stats_X, sz, aug_tfms=[RandomDihedral()], pad=sz//8)\n",
    "    test_tfms = tfms_for_test(test_stats_X,sz)\n",
    "    tfms += (test_tfms, )\n",
    "    print('\\n class to index mapping:\\n',lbl2index)\n",
    "    return create(tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(PATH,SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.trn_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect loaded data:\n",
    "\n",
    "Displaying the same image with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify which image-index\n",
    "idx = 1\n",
    "\n",
    "# loading it from GPU to CPU\n",
    "xx = x[idx].cpu().numpy().copy()\n",
    "yy = y[idx]\n",
    "# showing the image\n",
    "#\n",
    "#sp.axis('Off')\n",
    "#sp.set_title(\"Norm\", fontsize=11)\n",
    "figure, _ ,_ = tiff.imshow(np.sum(xx, axis=0))\n",
    "figure.set_size_inches(6,6)\n",
    "figure.add_subplot(111)\n",
    "\n",
    "figure2, _, _ = tiff.imshow(np.sum(data.trn_ds.denorm(xx,yy).squeeze() * 65536, axis=2)) # not very elegant atm. \n",
    "figure2.set_size_inches(6,6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet_with_Batchnorm\n",
    "\n",
    "Defining network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False, padding=1)\n",
    "        self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds  = x_chan.std (1)[:,None,None]\n",
    "        return (x-self.means) / self.stds *self.m + self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 5), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 8, cycle_len=1, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "x = learn.sched.plot_loss()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some more cylces - error & accuracy should continuously improve\n",
    "\n",
    "Note: cycle len = number of epochs per cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 8, wds=wd, cycle_len=10, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 2, wds=wd, cycle_len=20, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 2, wds=wd, cycle_len=20, use_clr=(20,8, 0.95, 0.85), best_save_name='YNet_Res_v5.1_redo_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Model evaluation\n",
    "\n",
    "This is one of the major areas that needs improvement in our workflow. The tools we have so far (confusion matrix and manual inpsection of images) are essential but definitely not sufficient to ensure that our model learns something biologicaly relevant. Ideas are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model:\n",
    "learn.load('Objective_A_Resnet_per_class_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-10, 1, wds=wd, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA() # run predictions with TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "cm = confusion_matrix(preds,y)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show random correct/incorrectly classified images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds_mean = np.mean(log_preds, axis=0) # averages predictions on original + 4 TTA images\n",
    "preds = np.argmax(log_preds_mean, axis=1) # converts into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = np.exp(log_preds_mean[:,0]) # prediction(WT)\n",
    "probs = np.exp(log_preds_mean) # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\n",
    "def rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, channel, figsize=(12,6), rows=1, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None: sp.set_title(titles[i], fontsize=11)\n",
    "        if channel is not None: plt.imshow(ims[i,channel,:,:]) \n",
    "        else: plt.imshow(np.sum(ims, axis=1)[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_with_title_from_ds_no_denorm(idxs, title, channel=None):\n",
    "    \n",
    "    imgs = np.stack(data.val_ds[x][0] for x in idxs) # get images by idx\n",
    "    corr_lbl = np.stack(data.val_ds[x][1] for x in idxs) # get correct label from data.val_ds by idx\n",
    "    pred_lbl = np.stack(preds[x] for x in idxs) # get predicted label from preds by idx\n",
    "    p_max = [np.amax(probs[x,:]) for x in idxs] # get highes probability from probs by idx\n",
    "    \n",
    "    title_fin = [f\"true = {corr_lbl[x]}\\n predicted: {pred_lbl[x]}\\n  p = {p_max[x]}\" for x in corr_lbl]\n",
    "    print(title)\n",
    "    \n",
    "    return plots(imgs, channel, rows=1, titles=title_fin, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot images according to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load from ds - not denormalized! \n",
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(True), \"Correctly classified\")\n",
    "#optionally pass channel arg. to select single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(False), \"Incorrectly classified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show most correct/incorrectly classified images per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_by_mask(mask, y, mult):\n",
    "    idxs = np.where(mask)[0]\n",
    "    return idxs[np.argsort(mult * probs[:,y][idxs])[:4]]\n",
    "\n",
    "def most_by_correct(y, is_correct): \n",
    "    mult = -1 if is_correct else 1\n",
    "    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), y, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, True), \"Most correctly classified WT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, False), \"Most incorrectly classified WT\") # logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, True), \"Most correctly classified mfb1KO\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, False), \"Most incorrectly classified mfb1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(2, True), \"Most correctly classified mfb1KO-mmr1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(3, True), \"Most correctly classified mmr1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show (most) uncertain images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_uncertain = t = np.argsort(np.amax(probs, axis = 1))[:6] # get best \"guess\" per image and list the least confident ones\n",
    "plot_val_with_title_from_ds_no_denorm(most_uncertain, \"Most uncertain predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
