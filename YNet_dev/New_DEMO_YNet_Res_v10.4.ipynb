{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YNet - Dataset 10.4:\n",
    "\n",
    "Data from Experiment (2), Mitochondria = Cit1-mCherry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import skimage.external.tifffile as tiff\n",
    "\n",
    "from common import Statistics, dataset_source\n",
    "from resources.conv_learner import *\n",
    "from resources.plots import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"datasets/yeast_v10.4/\"\n",
    "data_path = Path(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ('WT', 'mfb1KO', 'mmr1KO','mmr1KO-mfb1KO', 'dnm1KO', 'LatA_5uM', 'fzo1KO')\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "BATCH_SIZE = 64\n",
    "SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating normalization statistics\n",
    "\n",
    "Note that we are setting up train & val data, as well as test. Within test, we are here including a mutant cell type that the model never trains on. The idea is to use to the feature space developed during training to evaluate novel cell types by similarity to the landmarks that the model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: datasets\\yeast_v10.4\\train\\02_mfb1KO\n",
      "working on: datasets\\yeast_v10.4\\val\\02_mfb1KO\n",
      "working on: datasets\\yeast_v10.4\\train\\02_mmr1KO\n",
      "working on: datasets\\yeast_v10.4\\val\\02_mmr1KO\n",
      "working on: datasets\\yeast_v10.4\\train\\02_mmr1KO-mfb1KO\n",
      "working on: datasets\\yeast_v10.4\\val\\02_mmr1KO-mfb1KO\n",
      "working on: datasets\\yeast_v10.4\\train\\02_WT\n",
      "working on: datasets\\yeast_v10.4\\val\\02_WT\n",
      "working on: datasets\\yeast_v10.4\\train\\03_dnm1KO\n",
      "working on: datasets\\yeast_v10.4\\val\\03_dnm1KO\n",
      "working on: datasets\\yeast_v10.4\\train\\03_fzo1KO\n",
      "working on: datasets\\yeast_v10.4\\val\\03_fzo1KO\n",
      "working on: datasets\\yeast_v10.4\\train\\03_LatA-5uM\n",
      "working on: datasets\\yeast_v10.4\\val\\03_LatA-5uM\n",
      "working on: datasets\\yeast_v10.4\\train\\03_WT\n",
      "working on: datasets\\yeast_v10.4\\val\\03_WT\n",
      "working on: datasets\\yeast_v10.4\\train\\04_WT\n",
      "working on: datasets\\yeast_v10.4\\val\\04_WT\n",
      "working on: datasets\\yeast_v10.4\\test\\01_mfb1KO\n",
      "working on: datasets\\yeast_v10.4\\test\\01_mmr1KO\n",
      "working on: datasets\\yeast_v10.4\\test\\01_WT\n",
      "working on: datasets\\yeast_v10.4\\test\\03_axl1KO\n",
      "working on: datasets\\yeast_v10.4\\test\\03_bud1KO\n",
      "working on: datasets\\yeast_v10.4\\test\\03_DMSO\n",
      "working on: datasets\\yeast_v10.4\\test\\03_DTT\n",
      "working on: datasets\\yeast_v10.4\\test\\03_Eth\n",
      "working on: datasets\\yeast_v10.4\\test\\03_LatA-05uM\n"
     ]
    }
   ],
   "source": [
    "# stats_name = \"yeast_v10.2_per_class.dict\"\n",
    "classes = Statistics.source_class(data_path)\n",
    "\n",
    "train_val = zip(classes['train'], classes['val'])\n",
    "test_ = zip(classes['test'])\n",
    " \n",
    "main_stats = Statistics.per_class(train_val)\n",
    "test_stats = Statistics.per_class(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_mfb1KO: \t \t \t (array([0.00794, 0.00484]), array([0.00075, 0.00163]))\n",
      "02_mmr1KO: \t \t \t (array([0.00799, 0.00503]), array([0.0008 , 0.00186]))\n",
      "02_mmr1KO-mfb1KO: \t \t \t (array([0.00791, 0.00489]), array([0.00073, 0.00162]))\n",
      "02_WT: \t \t \t (array([0.00796, 0.00478]), array([0.00075, 0.00149]))\n",
      "03_dnm1KO: \t \t \t (array([0.02515, 0.00477]), array([0.0025 , 0.00192]))\n",
      "03_fzo1KO: \t \t \t (array([0.02517, 0.0047 ]), array([0.00202, 0.00202]))\n",
      "03_LatA-5uM: \t \t \t (array([0.0253, 0.0049]), array([0.0024, 0.0017]))\n",
      "03_WT: \t \t \t (array([0.02536, 0.00459]), array([0.00255, 0.00147]))\n",
      "04_WT: \t \t \t (array([0.02535, 0.00493]), array([0.00215, 0.00156]))\n"
     ]
    }
   ],
   "source": [
    "for keys in main_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {main_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_mfb1KO: \t \t \t (array([0.0211 , 0.00454]), array([0.00151, 0.00165]))\n",
      "01_mmr1KO: \t \t \t (array([0.02115, 0.00486]), array([0.00158, 0.00193]))\n",
      "01_WT: \t \t \t (array([0.0211 , 0.00449]), array([0.00149, 0.00129]))\n",
      "03_axl1KO: \t \t \t (array([0.02548, 0.00475]), array([0.00221, 0.00144]))\n",
      "03_bud1KO: \t \t \t (array([0.02544, 0.00459]), array([0.00223, 0.00142]))\n",
      "03_DMSO: \t \t \t (array([0.02535, 0.00494]), array([0.00216, 0.00156]))\n",
      "03_DTT: \t \t \t (array([0.02586, 0.00496]), array([0.00224, 0.00167]))\n",
      "03_Eth: \t \t \t (array([0.02533, 0.00469]), array([0.00226, 0.00134]))\n",
      "03_LatA-05uM: \t \t \t (array([0.02535, 0.00498]), array([0.00252, 0.00168]))\n"
     ]
    }
   ],
   "source": [
    "for keys in test_stats.keys():\n",
    "    print(f\"{keys}: \\t \\t \\t {test_stats[keys]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfms_for_test(stats, sz):\n",
    "    test_norm = Normalize(stats)\n",
    "    test_denorm = Denormalize(stats)\n",
    "    val_crop = CropType.NO\n",
    "    test_tfms = image_gen(test_norm, test_denorm,sz, crop_type=val_crop)\n",
    "    return test_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: str, sz, bs):\n",
    "    create, lbl2index, lbl2index_test = ImageClassifierData.prepare_from_path(path, val_name='val', bs=bs, num_workers=1,\n",
    "                                                                             test_name='test', test_with_labels=True, balance=False)\n",
    "    \n",
    "    main_stats_X = {lbl2index[key][0]: val for key, val in main_stats.items()}\n",
    "    tfms = tfms_from_stats(main_stats_X, sz, aug_tfms=[RandomDihedral()], pad=sz//8)\n",
    "    \n",
    "    test_stats_X = {lbl2index_test[key][0]: val for key, val in test_stats.items()}\n",
    "    test_tfms = tfms_for_test(test_stats_X,sz)\n",
    "    tfms += (test_tfms, )\n",
    "    \n",
    "#     print(main_stats_X)\n",
    "#     print(test_stats_X)\n",
    "    \n",
    "    print('\\n class to index mapping:\\n',lbl2index)\n",
    "    print('\\n class to index mapping:\\n',lbl2index_test)\n",
    "    return create(tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " class to index mapping:\n",
      " {'02_WT': [0, 0, 'WT'], '02_mfb1KO': [1, 1, 'mfb1KO'], '02_mmr1KO': [2, 2, 'mmr1KO'], '02_mmr1KO-mfb1KO': [3, 3, 'mmr1KO-mfb1KO'], '03_LatA-5uM': [4, 4, 'LatA-5uM'], '03_WT': [5, 0, 'WT'], '03_dnm1KO': [6, 5, 'dnm1KO'], '03_fzo1KO': [7, 6, 'fzo1KO'], '04_WT': [8, 0, 'WT']}\n",
      "\n",
      " class to index mapping:\n",
      " {'01_WT': [0, 0, 'WT'], '01_mfb1KO': [1, 1, 'mfb1KO'], '01_mmr1KO': [2, 2, 'mmr1KO'], '03_DMSO': [3, 3, 'DMSO'], '03_DTT': [4, 4, 'DTT'], '03_Eth': [5, 5, 'Eth'], '03_LatA-05uM': [6, 6, 'LatA-05uM'], '03_axl1KO': [7, 7, 'axl1KO'], '03_bud1KO': [8, 8, 'bud1KO']}\n"
     ]
    }
   ],
   "source": [
    "data = get_data(PATH,SIZE,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_composition():\n",
    "    \n",
    "    bat_ = iter(data.trn_dl)\n",
    "\n",
    "    for i in range(len(data.trn_dl)):\n",
    "        x, y = next(bat_)\n",
    "        ys = np.array([list(to_np(y)).count(j) for j in range(NUM_CLASSES)])\n",
    "        print\n",
    "        if i == 0:\n",
    "            bys = ys\n",
    "        else:\n",
    "            bys = np.vstack((bys, ys))\n",
    "\n",
    "    means = np.mean(bys, axis = 0)/64\n",
    "    print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3186  0.08232 0.1189  0.10823 0.08994 0.15549 0.11509]\n"
     ]
    }
   ],
   "source": [
    "analyze_batch_composition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(data.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data.val_dl.dataset.y))\n",
    "print(len(data.trn_dl.dataset.y))\n",
    "print(len(data.test_dl.dataset.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x20a22d973c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trn_dl.sampler#.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect loaded data:\n",
    "\n",
    "Displaying the same image with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify which image-index\n",
    "idx = 6\n",
    "\n",
    "# loading it from GPU to CPU\n",
    "xx = x[idx].cpu().numpy().copy()\n",
    "yy = y[idx]\n",
    "# showing the image\n",
    "#\n",
    "#sp.axis('Off')\n",
    "#sp.set_title(\"Norm\", fontsize=11)\n",
    "figure, _ ,_ = tiff.imshow(np.sum(xx, axis=0))\n",
    "figure.set_size_inches(6,6)\n",
    "figure.add_subplot(111)\n",
    "\n",
    "# figure2, _, _ = tiff.imshow(np.sum(data.trn_ds.denorm(xx,yy).squeeze() * 65536, axis=2)) # not very elegant atm. \n",
    "# figure2.set_size_inches(6,6)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet_with_Batchnorm\n",
    "\n",
    "Defining network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False, padding=1)\n",
    "        self.a = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.m = nn.Parameter(torch.ones(nf,1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            self.means = x_chan.mean(1)[:,None,None]\n",
    "            self.stds  = x_chan.std (1)[:,None,None]\n",
    "        return (x-self.means) / self.stds *self.m + self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLayer(BnLayer):\n",
    "    def forward(self, x): return x + super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, layers, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5, stride=1, padding=2)\n",
    "        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n",
    "            for i in range(len(layers) - 1)])\n",
    "        self.out = nn.Linear(layers[-1], c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for l,l2,l3 in zip(self.layers, self.layers2, self.layers3):\n",
    "            x = l3(l2(l(x)))\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-5 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 7), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6cfbcf47c04bb38f222ccf225b1e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 ---------------------------------------- STEP 0                                                                                                                                                              \n",
      "mean: [31.90244  7.70732 11.5122  10.43902  8.46341 15.14634 11.14634]\n",
      "stdev: [6.48377 2.9652  3.80065 3.813   3.47884 4.07587 4.57758]\n",
      "\n",
      "[WT]: 84.76%\n",
      "[mfb1KO]:  0.0%\n",
      "[mmr1KO]: 11.43%\n",
      "[mmr1KO-mfb1KO]:  0.0%\n",
      "[LatA-5uM]:  0.0%\n",
      "[dnm1KO]:  0.0%\n",
      "[fzo1KO]: 54.29%\n",
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      11.752529  4.601089   0.355556  \n",
      "EPOCH 1 ---------------------------------------- STEP 1                                                                                                                                                              \n",
      "mean: [31.90244  7.82927 11.46341 10.5122   8.4878  15.2439  11.07317]\n",
      "stdev: [6.83179 3.56748 3.62981 3.90198 3.67006 4.04716 4.84587]\n",
      "\n",
      "[WT]: 58.1%\n",
      "[mfb1KO]:  0.0%\n",
      "[mmr1KO]: 25.71%\n",
      "[mmr1KO-mfb1KO]: 8.571%\n",
      "[LatA-5uM]:  0.0%\n",
      "[dnm1KO]: 65.71%\n",
      "[fzo1KO]: 54.29%\n",
      "    1      4.881012   1.637538   0.365079  \n",
      "EPOCH 2 ---------------------------------------- STEP 2                                                                                                                                                              \n",
      "mean: [31.80488  7.65854 11.53659 10.39024  8.58537 15.2439  11.17073]\n",
      "stdev: [5.61453 2.8933  3.62308 3.84377 4.472   4.2586  3.75404]\n",
      "\n",
      "[WT]: 69.52%\n",
      "[mfb1KO]:  0.0%\n",
      "[mmr1KO]:  0.0%\n",
      "[mmr1KO-mfb1KO]:  0.0%\n",
      "[LatA-5uM]:  0.0%\n",
      "[dnm1KO]: 54.29%\n",
      "[fzo1KO]: 82.86%\n",
      "    2      2.801672   1.54317    0.384127  \n",
      "EPOCH 3 ---------------------------------------- STEP 3                                                                                                                                                              \n",
      "mean: [31.82927  7.7561  11.53659 10.39024  8.5122  15.2439  11.02439]\n",
      "stdev: [5.80539 3.01848 4.00059 3.6216  3.56214 3.91856 4.19342]\n",
      "\n",
      "[WT]: 75.24%\n",
      "[mfb1KO]:  0.0%\n",
      "[mmr1KO]:  0.0%\n",
      "[mmr1KO-mfb1KO]: 11.43%\n",
      "[LatA-5uM]: 2.857%\n",
      "[dnm1KO]: 57.14%\n",
      "[fzo1KO]: 88.57%\n",
      "    3      1.985208   1.342784   0.428571  \n",
      "EPOCH 4 ---------------------------------------- STEP 4                                                                                                                                                              \n",
      "mean: [31.65854  7.87805 11.36585 10.5122   8.56098 15.26829 11.09756]\n",
      "stdev: [5.28047 3.38717 4.51964 4.28339 3.70266 4.09059 4.73598]\n",
      "\n",
      "[WT]: 90.48%\n",
      "[mfb1KO]: 2.857%\n",
      "[mmr1KO]: 11.43%\n",
      "[mmr1KO-mfb1KO]:  0.0%\n",
      "[LatA-5uM]:  0.0%\n",
      "[dnm1KO]: 31.43%\n",
      "[fzo1KO]: 14.29%\n",
      "    4      1.626883   1.493069   0.368254  \n",
      "EPOCH 5 ---------------------------------------- STEP 5                                                                                                                                                              \n",
      "mean: [31.87805  7.82927 11.46341 10.46341  8.63415 15.26829 11.09756]\n",
      "stdev: [6.43616 3.16923 3.47182 3.8197  3.64093 4.85911 3.79219]\n",
      "\n",
      "[WT]: 70.48%\n",
      "[mfb1KO]: 5.714%\n",
      "[mmr1KO]: 22.86%\n",
      "[mmr1KO-mfb1KO]: 2.857%\n",
      "[LatA-5uM]:  0.0%\n",
      "[dnm1KO]: 62.86%\n",
      "[fzo1KO]: 85.71%\n",
      "    5      1.445794   1.366096   0.434921  \n",
      "EPOCH 6 ---------------------------------------- STEP 6                                                                                                                                                              \n",
      "mean: [31.73171  7.92683 11.4878  10.41463  8.65854 15.31707 11.     ]\n",
      "stdev: [4.84403 3.62505 4.55921 3.42142 3.46135 4.57159 4.08447]\n",
      "\n",
      "[WT]: 86.67%\n",
      "[mfb1KO]: 11.43%\n",
      "[mmr1KO]: 14.29%\n",
      "[mmr1KO-mfb1KO]: 34.29%\n",
      "[LatA-5uM]: 11.43%\n",
      "[dnm1KO]: 45.71%\n",
      "[fzo1KO]: 60.0%\n",
      "    6      1.367853   1.269066   0.485714  \n",
      "EPOCH 7 ---------------------------------------- STEP 7                                                                                                                                                              \n",
      "mean: [31.82927  7.80488 11.46341 10.34146  8.68293 15.41463 11.07317]\n",
      "stdev: [4.95788 3.18701 4.36224 4.84697 4.22789 3.83804 4.31328]\n",
      "\n",
      "[WT]: 71.43%\n",
      "[mfb1KO]: 11.43%\n",
      "[mmr1KO]: 42.86%\n",
      "[mmr1KO-mfb1KO]: 20.0%\n",
      "[LatA-5uM]: 11.43%\n",
      "[dnm1KO]: 57.14%\n",
      "[fzo1KO]: 77.14%\n",
      "    7      1.30229    1.324153   0.48254   \n",
      "\n",
      "Wall time: 3min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.32415]), 0.4825396772414919]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(1e-2, 8, cycle_len=1, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.39025000000001"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([31.80488, 7.65854, 11.53659, 10.39024,  8.58537, 15.2439,  11.17073])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "x = learn.sched.plot_loss()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some more cylces - error & accuracy should continuously improve\n",
    "\n",
    "Note: cycle len = number of epochs per cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 10, wds=wd, cycle_len=2, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 3, wds=wd, cycle_len=2, cycle_mult = 2, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-4 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-3, 3, wds=wd, cycle_len=2, cycle_mult=2, use_clr=(20,8, 0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-4 # weight-decay/L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time learn.fit(1e-2, 2, wds=wd, cycle_len=2, cycle_mult=2, use_clr=(20,8, 0.95, 0.85)) #, best_save_name = 'YNet_Res_v10.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Model evaluation\n",
    "\n",
    "This is one of the major areas that needs improvement in our workflow. The tools we have so far (confusion matrix and manual inpsection of images) are essential but definitely not sufficient to ensure that our model learns something biologicaly relevant. Ideas are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('YNet_Res_v10.4_release_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model:\n",
    "learn.load('YNet_Res_v10.3_release_1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time learn.fit(1e-10, 1, wds=wd, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.warm_up(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(n_aug=4) # run predictions with TTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix \n",
    "log_preds_mean = np.mean(log_preds, axis=0)\n",
    "preds = np.argmax(log_preds_mean, axis=1)\n",
    "cm = confusion_matrix(preds,y)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-set eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing train and test datasets as exposed by dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Yinan, please take the functionality of the next 2 cells and transfer it to the data_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_ = data.trn_dl\n",
    "batch_ = iter(dl_)\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for b in range(0,len(dl_)):\n",
    "    x_, y_ = next(batch_)\n",
    "\n",
    "    x_np = to_np(x_)\n",
    "    y_np = to_np(y_)\n",
    "    \n",
    "    im_means = np.mean(x_np, axis=(2,3))\n",
    "    \n",
    "    ax.plot(im_means[:,0], im_means[:,1], 'o', color = 'C0' , alpha=0.5)\n",
    "        \n",
    "plt.xlim(-0.4,0.4)\n",
    "plt.ylim(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_t = data.test_dl\n",
    "batch_t = iter(dl_t)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for b in range(0,len(dl_t)):\n",
    "    x_, y_ = next(batch_t)\n",
    "\n",
    "    x_np = to_np(x_)\n",
    "    y_np = to_np(y_)\n",
    "    \n",
    "    im_means = np.mean(x_np, axis=(2,3))\n",
    "    \n",
    "    ax.plot(im_means[:,0], im_means[:,1], 'o', color = 'C1' , alpha=0.5)\n",
    "    \n",
    "plt.xlim(-0.4,0.4)\n",
    "plt.ylim(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_preds, targs = learn.predict_with_targs(is_test=True)\n",
    "testprobs = np.exp(test_log_preds)\n",
    "preds = np.argmax(testprobs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @James, please transfer the functionality of the next 5 cells into the data_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @James, there is definitely a simpler way of generating test_lbl2idx_ than calling this entire line. Please trim it down. \n",
    "\n",
    "_, lbl2idx_, test_lbl2idx_ = ImageClassifierData.prepare_from_path(PATH, val_name='val', bs=64, num_workers=1, test_name='test', test_with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions dictionary\n",
    "\n",
    "h = 0\n",
    "preds_dict = {}\n",
    "for i, key in enumerate(test_lbl2idx_.keys()):\n",
    "    l = h\n",
    "    h = h + list(data.test_dl.dataset.src_idx).count(i)\n",
    "    preds_dict[key] = list(preds[l:h])\n",
    "    print(f\"{key} predictions ready ({h - l} elements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rel = {}\n",
    "for key in preds_dict.keys():\n",
    "    val = {cls: preds_dict[key].count(i)/len(preds_dict[key]) for i, cls in enumerate(data.classes)}\n",
    "    preds_rel[key]= val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_preds(targets, preds_rel):\n",
    "    \n",
    "    if not isinstance(targets, list):\n",
    "        targets = [targets]\n",
    "        \n",
    "    x = math.ceil((int(len(targets)) /2)) # dynamic scaling of GridSpec\n",
    "    sz = 4 * x # dynamic scaling of figuresize\n",
    "    \n",
    "    # plotting:\n",
    "    plt.figure(figsize=(12,sz))\n",
    "    gs1 = plt.GridSpec(x,2)\n",
    "    gs1.update(wspace = 0.4)\n",
    "\n",
    "    for i, targ in enumerate(targets):\n",
    "        to_plot = [preds_rel[targ][key] for key in data.classes] # extracting data\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.barh(data.classes, to_plot)\n",
    "        ax1.set_title(targ)\n",
    "        ax1.set_xlim(0,1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = list(test_lbl2idx_.keys())\n",
    "\n",
    "plot_test_preds(test_classes, preds_rel)\n",
    "# plot_test_preds(['01_WT', '03_WT', '03_fzo1KO', '01_mfb1KO'], preds_rel)\n",
    "# plot_test_preds(['01_WT'], preds_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show random correct/incorrectly classified images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds_mean = np.mean(log_preds, axis=0) # averages predictions on original + 4 TTA images\n",
    "preds = np.argmax(log_preds_mean, axis=1) # converts into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = np.exp(log_preds_mean[:,0]) # prediction(WT)\n",
    "probs = np.exp(log_preds_mean) # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\n",
    "def rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, channel, figsize=(12,6), rows=1, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None: sp.set_title(titles[i], fontsize=11)\n",
    "        if channel is not None: plt.imshow(ims[i,channel,:,:]) \n",
    "        else: plt.imshow(np.sum(ims, axis=1)[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_val_with_title_from_ds_no_denorm(idxs, title, channel=None):\n",
    "    \n",
    "    imgs = np.stack(data.val_ds[x][0] for x in idxs) # get images by idx\n",
    "    corr_lbl = np.stack(data.val_ds[x][1] for x in idxs) # get correct label from data.val_ds by idx\n",
    "    pred_lbl = np.stack(preds[x] for x in idxs) # get predicted label from preds by idx\n",
    "    p_max = [np.amax(probs[x,:]) for x in idxs] # get highes probability from probs by idx\n",
    "    \n",
    "    title_fin = [f\"true = {corr_lbl[x]}\\n predicted: {pred_lbl[x]}\\n  p = {p_max[x]}\" for x in corr_lbl]\n",
    "    print(title)\n",
    "    \n",
    "    return plots(imgs, channel, rows=1, titles=title_fin, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot images according to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load from ds - not denormalized! \n",
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(True), \"Correctly classified\")\n",
    "#optionally pass channel arg. to select single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(rand_by_correct(False), \"Incorrectly classified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show most correct/incorrectly classified images per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_by_mask(mask, y, mult):\n",
    "    idxs = np.where(mask)[0]\n",
    "    return idxs[np.argsort(mult * probs[:,y][idxs])[:4]]\n",
    "\n",
    "def most_by_correct(y, is_correct): \n",
    "    mult = -1 if is_correct else 1\n",
    "    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), y, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, True), \"Most correctly classified WT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(0, False), \"Most incorrectly classified WT\") # logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, True), \"Most correctly classified mfb1KO\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(1, False), \"Most incorrectly classified mfb1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(2, True), \"Most correctly classified mfb1KO-mmr1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_val_with_title_from_ds_no_denorm(most_by_correct(3, True), \"Most correctly classified mmr1KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show (most) uncertain images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_uncertain = t = np.argsort(np.amax(probs, axis = 1))[:6] # get best \"guess\" per image and list the least confident ones\n",
    "plot_val_with_title_from_ds_no_denorm(most_uncertain, \"Most uncertain predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
