{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from MLDataTools.image_normalization import RandomDihedral\n",
    "from skimage.external import tifffile as tiff\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# torch setup\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor) # so it doesnt throw a incompatible type exception\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # important for cloud compatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard setup\n",
    "from datetime import datetime\n",
    "date = datetime.now().strftime(\"%Y-%m-%d.%H:%M:%S\")\n",
    "\n",
    "writer = SummaryWriter('tensorboardx/ResNet50_'+date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = net.fc.in_features\n",
    "net.fc = nn.Linear(in_features, 4) # 4 = num classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToImage:\n",
    "    def __call__(self, sample):\n",
    "        sample = torch.from_numpy(sample)\n",
    "        zeros = torch.zeros(1,200,200)\n",
    "        return torch.cat((sample, zeros))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_transforms = transforms.Compose([\n",
    "    ToImage(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    RandomDihedral(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets us control the datatype when the image is being read\n",
    "def tiff_read(path:str):\n",
    "    image = tiff.imread(path).astype(np.float)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "DATA_ROOT = '/home/user/HDev Dropbox/Projects/YNet_ready_data/yeast_v4'\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(DATA_ROOT+'/train', transform=ds_transforms, loader=tiff_read)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(DATA_ROOT+'/test', transform=ds_transforms, loader=tiff_read)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# invert class_to_id\n",
    "idx_to_class = {v:k for k,v in trainset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing procedures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train() # affects only modules like Dropout\n",
    "    trainiter = iter(trainloader)\n",
    "    for batch_idx, (data, targets) in enumerate(trainiter, 0):\n",
    "        # get the inputs\n",
    "\n",
    "        data, targets = Variable(data), Variable(targets)\n",
    "        l = data.size(0)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad() # dont forget to do that\n",
    "        output = net(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        # tensorboard\n",
    "        global global_step\n",
    "        global_step += 1\n",
    "        writer.add_scalar('Train_Loss', loss.data.item(), global_step)\n",
    "        if batch_idx % 2 == 0: # every 2nd batch add to our embedding writer\n",
    "            targets = targets.type(torch.DoubleTensor)\n",
    "            writer.add_embedding(output, metadata=targets.data, label_img=data.data, global_step=global_step)\n",
    "        if batch_idx % 5 == 0:\n",
    "            samples_done = batch_idx * BATCH_SIZE\n",
    "            percent = 100. * samples_donne / len(trainloader.dataset)\n",
    "            print(f\"Train Epoch: {epoch} [{samples_done}/{len(trainloader.dataset)} \"\n",
    "                  f\"({percent:8.5}%)]\\tLoss: {loss.item():10.5}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        wcases = [] # list of worst cases\n",
    "        classes_correct = list(0 for i in range(NUM_CLASSES))\n",
    "        classes_total = list(0 for i in range(NUM_CLASSES))\n",
    "        \n",
    "        for data, targets in iter(testloader):\n",
    "            # NOTE: UPDATE THIS LINE WHEN TORCH VERSION == 0.4\n",
    "            data, targets = Variable(data), Variable(targets)\n",
    "            \n",
    "            output = net(data)\n",
    "            # errors is a top 2 List[error:float, index of sample:int]\n",
    "            errors = worst_cases(output, targets)  \n",
    "            \n",
    "            for error, idx in errors:\n",
    "                target = targets[idx].data.item()\n",
    "                label = f\"{idx_to_class[target]}_{error:10.5}\"\n",
    "                tensor = data[idx].numpy().copy()\n",
    "                wcases.append((tensor,label))\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += criterion(output, targets).data.item()\n",
    "            \n",
    "            \n",
    "            # get the index of the max log-probability\n",
    "            _, pred = output.max(1) # returns a tuple the last element is the index Tensor\n",
    "            c = (pred == targets).squeeze()\n",
    "            l = c.size(0) # to account for different batch sizes\n",
    "            # this helps to identify which classes the network is struggling with\n",
    "            for i in range(l):\n",
    "                label = targets.data[i].item()\n",
    "                classes_correct[label] += c.data[i].item()\n",
    "                classes_total[label] += 1\n",
    "                        \n",
    "        writer.add_scalar('Test_Loss', test_loss, epoch)\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        accuracy = 100. * (sum(classes_correct) / sum(classes_total))\n",
    "        print(f\"\\nTest set: Average loss: {test_loss:6.5}, Accuracy: {accuracy:10.5}\\n\")\n",
    "        \n",
    "        for i, total, correct in zip(range(NUM_CLASSES), classes_total, classes_correct):\n",
    "            cl = idx_to_class[i]\n",
    "            cl_accuracy = 100. * (classes_correct[i] / classes_total[i])\n",
    "            print(f\"class [{cl}]: accuracy {cl_accuracy:10.4}%\")\n",
    "        print()# prints a newline\n",
    "        \n",
    "#         for image, label in wcases[:5]: # List[image:np.array, class:str]\n",
    "#             # should be of dimensions (2, 200, 200)\n",
    "#             tiff.imshow(image, title=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps to identify which cases the network was really wrong about\n",
    "def worst_cases(output: torch.Tensor, targets: torch.Tensor, top=2):\n",
    "    assert output.size(0) == targets.size(0)\n",
    "    length = output.size(0)\n",
    "    errors = []\n",
    "    for i in range(length):\n",
    "        z = torch.zeros(NUM_CLASSES)\n",
    "        label = targets[i].item()\n",
    "        z[label] = 1\n",
    "        diff = (output[i] - z).numpy().copy()\n",
    "        diff = np.sum(np.abs(diff))\n",
    "        errors.append((diff, i))\n",
    "        \n",
    "    errors.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    return errors[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples_donne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-821b0ff57595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-95e877e751fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0msamples_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpercent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msamples_donne\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             print(f\"Train Epoch: {epoch} [{samples_done}/{len(trainloader.dataset)} \"\n\u001b[1;32m     29\u001b[0m                   f\"({percent:8.5}%)]\\tLoss: {loss.item():10.5}\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples_donne' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "global_step = 0\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train(i)\n",
    "    test(i)\n",
    "    \n",
    "print(\"\\n Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
