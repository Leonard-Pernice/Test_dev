{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from models import CNN\n",
    "from pathlib import Path\n",
    "from MLDataTools.image_normalization import RandomDihedral\n",
    "from skimage.external import tifffile as tiff\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# torch setup\n",
    "torch.set_default_tensor_type(torch.DoubleTensor) # so it doesnt throw a incompatible type exception\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # important for cloud compatability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard to visualise class embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard setup\n",
    "writer = SummaryWriter('tensorboardx/SimpleCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the train loader and the test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/Users/cerber/HDev Dropbox/Projects/YNet_ready_data/yeast_v4'\n",
    "data_path = Path(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiff_read(path:str):\n",
    "    image = tiff.imread(path).astype(np.double)\n",
    "    return image\n",
    "\n",
    "class GetInfo:\n",
    "    def __init__(self, label=None):\n",
    "        self.label = label\n",
    "    def __call__(self, sample):\n",
    "        try:\n",
    "            print(sample.shape)\n",
    "        except: pass\n",
    "        finally:\n",
    "            if self.label: print(self.label)\n",
    "            print(type(sample))\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_transforms = transforms.Compose([\n",
    "    RandomDihedral()\n",
    "])\n",
    "trainset = torchvision.datasets.ImageFolder(DATA_ROOT+'/train', transform=ds_transforms, loader=tiff_read)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=40, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(DATA_ROOT+'/test', transform=ds_transforms, loader=tiff_read)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=40, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_name: (mean, stdev)\n",
    "stats = {\n",
    "    'WT': ([48.37304926, 95.64728183],\n",
    "  [521.95544101, 310.77445807]),\n",
    "    'mfb1KO': ([ 47.58621839, 102.40188124],\n",
    "  [520.43241635, 311.95406937]),\n",
    "    'mfb1KO_mmr1KO': ([ 47.79873863, 100.28439551],\n",
    "  [517.82433373, 310.53787264]),\n",
    "    'mmr1KO': ([ 49.22677943, 110.97112597],\n",
    "  [522.00261751, 315.86275802])\n",
    "}\n",
    "\n",
    "# invert class_to_id\n",
    "idx_to_class = {v:k for k,v in trainset.class_to_idx.items()}\n",
    "norm_transforms = {} # class_id: normalization_transformation for that class\n",
    "for key, value in stats.items():\n",
    "    label = trainset.class_to_idx[key]\n",
    "    norm_transforms[label] = transforms.Normalize(value[0],value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(32 * 47 * 47, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 32 * 47 * 47)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train() # affects only modules like Dropout\n",
    "    trainiter = iter(trainloader)\n",
    "    for batch_idx, (data, targets) in enumerate(trainiter, 0):\n",
    "        # get the inputs\n",
    "        for i, target in enumerate(targets, 0): # normalize the inputs according to class\n",
    "            t = norm_transforms[target.item()]\n",
    "            data[i] = t(data[i])\n",
    "\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # backprop\n",
    "        optimizer.zero_grad() # dont forget to do that\n",
    "        output = net(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # tensorboard\n",
    "        global global_step\n",
    "        global_step += 1\n",
    "        writer.add_scalar('Train_Loss', loss.item(), global_step)\n",
    "        if batch_idx % 2 == 0: # every 2nd batch add to our embedding writer\n",
    "            targets = targets.type(torch.DoubleTensor)\n",
    "            data = torch.cat((data.data,torch.zeros(40,1,200,200)),dim=1)\n",
    "            writer.add_embedding(output, metadata=targets.data, label_img=data.data, global_step=global_step)\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)} \"\n",
    "                  f\"({100. * batch_idx / len(trainloader)}%)]\\tLoss: {loss.item()}\")\n",
    "#\n",
    "# A simple test procedure to measure CNN the performances on MNIST.\n",
    "#\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, targets in iter(testloader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            for i, target in enumerate(targets, 0): # normalize the inputs according to class\n",
    "                t = norm_transforms[target.item()]\n",
    "                data[i] = t(data[i])\n",
    "            output = net(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += criterion(output, targets).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        \n",
    "        writer.add_scalar('Test_Loss', test_loss, epoch)\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        print(f\"\\nTest set: Average loss: {test_loss:{5}}, Accuracy: {correct}/{len(testloader.dataset)}\"\n",
    "              f\" ({100. * correct / len(testloader.dataset):{5}}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Train Epoch: 0 [0/609 (0.0%)]\tLoss: 1.3673398858655947\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Train Epoch: 0 [200/609 (31.25%)]\tLoss: 1.414996474859786\n",
      "Train Epoch: 0 [400/609 (62.5%)]\tLoss: 1.3557049445603875\n",
      "Train Epoch: 0 [135/609 (93.75%)]\tLoss: 1.1299899743844208\n",
      "\n",
      "Test set: Average loss: 0.040172739227337835, Accuracy: 24/105 (22.857142857142858%)\n",
      "\n",
      "Train Epoch: 1 [0/609 (0.0%)]\tLoss: 1.2239306553884406\n",
      "Train Epoch: 1 [200/609 (31.25%)]\tLoss: 1.5041359900483584\n",
      "Train Epoch: 1 [400/609 (62.5%)]\tLoss: 1.4174383217488589\n",
      "Train Epoch: 1 [135/609 (93.75%)]\tLoss: 1.4196431991152931\n",
      "\n",
      "Test set: Average loss: 0.03931904363105184, Accuracy: 24/105 (22.857142857142858%)\n",
      "\n",
      "Train Epoch: 2 [0/609 (0.0%)]\tLoss: 1.3685422720516034\n",
      "Train Epoch: 2 [200/609 (31.25%)]\tLoss: 1.3504726639667521\n",
      "Train Epoch: 2 [400/609 (62.5%)]\tLoss: 1.3123645755797522\n",
      "Train Epoch: 2 [135/609 (93.75%)]\tLoss: 1.3112037324048331\n",
      "\n",
      "Test set: Average loss: 0.038924862981101954, Accuracy: 24/105 (22.857142857142858%)\n",
      "\n",
      "Train Epoch: 3 [0/609 (0.0%)]\tLoss: 1.3577603018263904\n",
      "Train Epoch: 3 [200/609 (31.25%)]\tLoss: 1.4168709278764235\n",
      "Train Epoch: 3 [400/609 (62.5%)]\tLoss: 1.351486412631139\n",
      "Train Epoch: 3 [135/609 (93.75%)]\tLoss: 1.3058989415684787\n",
      "\n",
      "Test set: Average loss: 0.03828479568239784, Accuracy: 29/105 (27.61904761904762%)\n",
      "\n",
      "Train Epoch: 4 [0/609 (0.0%)]\tLoss: 1.3989131286220193\n",
      "Train Epoch: 4 [200/609 (31.25%)]\tLoss: 1.3231905813759959\n",
      "Train Epoch: 4 [400/609 (62.5%)]\tLoss: 1.339927246256496\n",
      "Train Epoch: 4 [135/609 (93.75%)]\tLoss: 1.3698262019404477\n",
      "\n",
      "Test set: Average loss: 0.0381698516066194, Accuracy: 37/105 (35.23809523809524%)\n",
      "\n",
      "Train Epoch: 5 [0/609 (0.0%)]\tLoss: 1.2697385893976456\n",
      "Train Epoch: 5 [200/609 (31.25%)]\tLoss: 1.2883352856918795\n",
      "Train Epoch: 5 [400/609 (62.5%)]\tLoss: 1.324665243905634\n",
      "Train Epoch: 5 [135/609 (93.75%)]\tLoss: 1.1094628025690116\n",
      "\n",
      "Test set: Average loss: 0.0371580094569948, Accuracy: 35/105 (33.333333333333336%)\n",
      "\n",
      "Train Epoch: 6 [0/609 (0.0%)]\tLoss: 1.1664133977585978\n",
      "Train Epoch: 6 [200/609 (31.25%)]\tLoss: 1.3401315495118842\n",
      "Train Epoch: 6 [400/609 (62.5%)]\tLoss: 1.2315370522981053\n",
      "Train Epoch: 6 [135/609 (93.75%)]\tLoss: 1.1638485400875547\n",
      "\n",
      "Test set: Average loss: 0.03681738920128722, Accuracy: 40/105 (38.095238095238095%)\n",
      "\n",
      "Train Epoch: 7 [0/609 (0.0%)]\tLoss: 1.2292001698429773\n",
      "Train Epoch: 7 [200/609 (31.25%)]\tLoss: 1.18480611709114\n",
      "Train Epoch: 7 [400/609 (62.5%)]\tLoss: 1.1832716326304877\n",
      "Train Epoch: 7 [135/609 (93.75%)]\tLoss: 1.232173986362547\n",
      "\n",
      "Test set: Average loss: 0.03642890387825504, Accuracy: 43/105 (40.95238095238095%)\n",
      "\n",
      "Train Epoch: 8 [0/609 (0.0%)]\tLoss: 1.2095199129976857\n",
      "Train Epoch: 8 [200/609 (31.25%)]\tLoss: 1.2838062815883093\n",
      "Train Epoch: 8 [400/609 (62.5%)]\tLoss: 1.2959568648781326\n",
      "Train Epoch: 8 [135/609 (93.75%)]\tLoss: 0.9942230398833478\n",
      "\n",
      "Test set: Average loss: 0.03559371610717637, Accuracy: 44/105 (41.904761904761905%)\n",
      "\n",
      "Train Epoch: 9 [0/609 (0.0%)]\tLoss: 1.2234018472713941\n",
      "Train Epoch: 9 [200/609 (31.25%)]\tLoss: 1.04028429377106\n",
      "Train Epoch: 9 [400/609 (62.5%)]\tLoss: 0.9775334039683112\n",
      "Train Epoch: 9 [135/609 (93.75%)]\tLoss: 1.1112990444862307\n",
      "\n",
      "Test set: Average loss: 0.035570738359245485, Accuracy: 43/105 (40.95238095238095%)\n",
      "\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "global_step = 0\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train(i)\n",
    "    test(i)\n",
    "    \n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WT': 0, 'mfb1KO': 1, 'mfb1KO_mmr1KO': 2, 'mmr1KO': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
